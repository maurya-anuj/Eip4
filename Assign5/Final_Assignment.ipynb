{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maurya-anuj/Eip4/blob/master/Assign5/Final_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxG3tULh7LYA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "28c85720-03b9-4288-815c-85654433949e"
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/Deep Vision/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX9N9bFi7fP1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9640849b-8669-4631-a1b1-c8285c49a2ca"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "# import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# from functools import partial\n",
        "# from pathlib import Path \n",
        "# from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "# from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "# from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "# from keras.optimizers import SGD\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "#########################\n",
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()\n",
        "\n",
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "\n",
        "##########  AUGUMENTATION GEN.... CUTOUT  #####################\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augumentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augumentation = augumentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        image = image/255\n",
        "        if self.augumentation is not None:\n",
        "            image = self.augumentation.flow(image, shuffle=False).next()\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15, random_state=1)\n",
        "print(train_df.shape, val_df.shape)\n",
        "\n",
        "########################################################\n",
        "pixel_level = False\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser\n",
        "\n",
        "########################################\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "aug = ImageDataGenerator(\n",
        "        # # set input mean to 0 over the dataset\n",
        "        # featurewise_center=False,\n",
        "        # # set each sample mean to 0\n",
        "        # samplewise_center=False,\n",
        "        # # divide inputs by std of dataset\n",
        "        # featurewise_std_normalization=False,\n",
        "        # # divide each input by its std\n",
        "        # samplewise_std_normalization=False,\n",
        "        # # apply ZCA whitening\n",
        "        # zca_whitening=False,\n",
        "        # # epsilon for ZCA whitening\n",
        "        # zca_epsilon=1e-06,\n",
        "        # # randomly rotate images in the range (deg 0 to 180)\n",
        "        # rotation_range=10,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # # set range for random shear\n",
        "        # shear_range=0.,\n",
        "        # # set range for random zoom\n",
        "        # zoom_range=0.1,\n",
        "        # # set range for random channel shifts\n",
        "        # channel_shift_range=0.,\n",
        "        # # set mode for filling points outside the input boundaries\n",
        "        # fill_mode='nearest',\n",
        "        # # value used for fill_mode = \"constant\"\n",
        "        # cval=0.,\n",
        "        # # randomly flip images\n",
        "        # horizontal_flip=True,\n",
        "        # # randomly flip images\n",
        "        # vertical_flip=False,\n",
        "        # # set rescaling factor (applied before any other transformation)\n",
        "        # rescale=1.0/255.0,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=pixel_level),\n",
        "        # # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        # data_format=None,\n",
        "        # # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32, augumentation = aug)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False, augumentation = None)  # val_df\n",
        "\n",
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "\n",
        "#########################################################################################\n",
        "from keras.models import model_from_json\n",
        "json_file = open(\"/content/gdrive/My Drive/Deep Vision/resnet34_new.json\", 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model = model_from_json(loaded_model_json)\n",
        "model.layers.pop() # Remove last dense layer\n",
        "\n",
        "neck = model.output\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.3)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "model = Model(\n",
        "    inputs=model.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")\n",
        "\n",
        "####################  COMPILE  ############################\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "import os\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "model_save = keras.callbacks.ModelCheckpoint('model{epoch:08d}.h5', period=5) \n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=.05,\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               verbose=1,\n",
        "                               min_delta=0.01,\n",
        "                               min_lr=0.5e-6)\n",
        "def lr_schedule(epoch):\n",
        "    lr = 1e-3\n",
        "    if epoch > 40:\n",
        "        lr *= 0.5e-2\n",
        "    elif epoch > 30:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 20:\n",
        "        lr *= 0.5e-1\n",
        "    elif epoch > 10:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "callbacks = [lr_scheduler, model_save]\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "opt = SGD(lr=0.0001)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    # loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "######################  FIT  #######################################\n",
        "model_info = model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks = callbacks\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11537, 28) (2036, 28)\n",
            "Epoch 1/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 187s 520ms/step - loss: 10.8877 - gender_output_loss: 0.6886 - image_quality_output_loss: 1.0758 - age_output_loss: 1.5842 - weight_output_loss: 1.3265 - bag_output_loss: 1.0680 - footwear_output_loss: 1.0905 - pose_output_loss: 1.0565 - emotion_output_loss: 1.2957 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5411 - age_output_acc: 0.3807 - weight_output_acc: 0.6096 - bag_output_acc: 0.5492 - footwear_output_acc: 0.3950 - pose_output_acc: 0.6011 - emotion_output_acc: 0.6771 - val_loss: 10.6581 - val_gender_output_loss: 0.6873 - val_image_quality_output_loss: 1.0553 - val_age_output_loss: 1.5689 - val_weight_output_loss: 1.2717 - val_bag_output_loss: 1.0429 - val_footwear_output_loss: 1.0822 - val_pose_output_loss: 1.0236 - val_emotion_output_loss: 1.2244 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4451 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 1/50\n",
            "Epoch 2/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 159s 442ms/step - loss: 10.4541 - gender_output_loss: 0.6868 - image_quality_output_loss: 1.0415 - age_output_loss: 1.5485 - weight_output_loss: 1.2281 - bag_output_loss: 1.0209 - footwear_output_loss: 1.0741 - pose_output_loss: 1.0011 - emotion_output_loss: 1.1516 - gender_output_acc: 0.5624 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4046 - weight_output_acc: 0.6347 - bag_output_acc: 0.5668 - footwear_output_acc: 0.4422 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7161 - val_loss: 10.3120 - val_gender_output_loss: 0.6861 - val_image_quality_output_loss: 1.0280 - val_age_output_loss: 1.5394 - val_weight_output_loss: 1.1823 - val_bag_output_loss: 1.0086 - val_footwear_output_loss: 1.0693 - val_pose_output_loss: 0.9817 - val_emotion_output_loss: 1.1152 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 3/50\n",
            "Learning rate:  0.001\n",
            "359/360 [============================>.] - ETA: 0s - loss: 10.1521 - gender_output_loss: 0.6863 - image_quality_output_loss: 1.0191 - age_output_loss: 1.5200 - weight_output_loss: 1.1528 - bag_output_loss: 0.9897 - footwear_output_loss: 1.0632 - pose_output_loss: 0.9684 - emotion_output_loss: 1.0513 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4038 - weight_output_acc: 0.6345 - bag_output_acc: 0.5669 - footwear_output_acc: 0.4454 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7164Epoch 3/50\n",
            "360/360 [==============================] - 158s 439ms/step - loss: 10.1517 - gender_output_loss: 0.6863 - image_quality_output_loss: 1.0192 - age_output_loss: 1.5198 - weight_output_loss: 1.1527 - bag_output_loss: 0.9898 - footwear_output_loss: 1.0631 - pose_output_loss: 0.9683 - emotion_output_loss: 1.0511 - gender_output_acc: 0.5624 - image_quality_output_acc: 0.5527 - age_output_acc: 0.4043 - weight_output_acc: 0.6346 - bag_output_acc: 0.5664 - footwear_output_acc: 0.4455 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7165 - val_loss: 10.0830 - val_gender_output_loss: 0.6856 - val_image_quality_output_loss: 1.0104 - val_age_output_loss: 1.5163 - val_weight_output_loss: 1.1192 - val_bag_output_loss: 0.9861 - val_footwear_output_loss: 1.0608 - val_pose_output_loss: 0.9575 - val_emotion_output_loss: 1.0458 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 4/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 156s 435ms/step - loss: 9.9491 - gender_output_loss: 0.6856 - image_quality_output_loss: 1.0046 - age_output_loss: 1.4976 - weight_output_loss: 1.0993 - bag_output_loss: 0.9684 - footwear_output_loss: 1.0555 - pose_output_loss: 0.9492 - emotion_output_loss: 0.9877 - gender_output_acc: 0.5622 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4040 - weight_output_acc: 0.6350 - bag_output_acc: 0.5664 - footwear_output_acc: 0.4456 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7160 - val_loss: 9.9340 - val_gender_output_loss: 0.6853 - val_image_quality_output_loss: 0.9991 - val_age_output_loss: 1.4988 - val_weight_output_loss: 1.0749 - val_bag_output_loss: 0.9711 - val_footwear_output_loss: 1.0553 - val_pose_output_loss: 0.9435 - val_emotion_output_loss: 1.0051 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 5/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 158s 439ms/step - loss: 9.8163 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9955 - age_output_loss: 1.4797 - weight_output_loss: 1.0629 - bag_output_loss: 0.9536 - footwear_output_loss: 1.0503 - pose_output_loss: 0.9387 - emotion_output_loss: 0.9492 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4043 - weight_output_acc: 0.6346 - bag_output_acc: 0.5665 - footwear_output_acc: 0.4452 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 9.8404 - val_gender_output_loss: 0.6852 - val_image_quality_output_loss: 0.9921 - val_age_output_loss: 1.4854 - val_weight_output_loss: 1.0454 - val_bag_output_loss: 0.9609 - val_footwear_output_loss: 1.0516 - val_pose_output_loss: 0.9360 - val_emotion_output_loss: 0.9830 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 6/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 156s 433ms/step - loss: 9.7315 - gender_output_loss: 0.6856 - image_quality_output_loss: 0.9900 - age_output_loss: 1.4666 - weight_output_loss: 1.0391 - bag_output_loss: 0.9436 - footwear_output_loss: 1.0469 - pose_output_loss: 0.9326 - emotion_output_loss: 0.9265 - gender_output_acc: 0.5623 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4040 - weight_output_acc: 0.6345 - bag_output_acc: 0.5663 - footwear_output_acc: 0.4452 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7165 - val_loss: 9.7822 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9878 - val_age_output_loss: 1.4756 - val_weight_output_loss: 1.0260 - val_bag_output_loss: 0.9540 - val_footwear_output_loss: 1.0491 - val_pose_output_loss: 0.9319 - val_emotion_output_loss: 0.9722 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 7/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 160s 445ms/step - loss: 9.6764 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9864 - age_output_loss: 1.4561 - weight_output_loss: 1.0232 - bag_output_loss: 0.9360 - footwear_output_loss: 1.0440 - pose_output_loss: 0.9295 - emotion_output_loss: 0.9153 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4042 - weight_output_acc: 0.6345 - bag_output_acc: 0.5668 - footwear_output_acc: 0.4452 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7159 - val_loss: 9.7458 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9852 - val_age_output_loss: 1.4682 - val_weight_output_loss: 1.0134 - val_bag_output_loss: 0.9491 - val_footwear_output_loss: 1.0474 - val_pose_output_loss: 0.9298 - val_emotion_output_loss: 0.9673 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 8/50\n",
            "Learning rate:  0.001\n",
            "359/360 [============================>.] - ETA: 0s - loss: 9.6398 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9846 - age_output_loss: 1.4485 - weight_output_loss: 1.0125 - bag_output_loss: 0.9306 - footwear_output_loss: 1.0425 - pose_output_loss: 0.9278 - emotion_output_loss: 0.9078 - gender_output_acc: 0.5627 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4044 - weight_output_acc: 0.6345 - bag_output_acc: 0.5667 - footwear_output_acc: 0.4452 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7162Learning rate:  0.001\n",
            "360/360 [==============================] - 161s 446ms/step - loss: 9.6397 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9845 - age_output_loss: 1.4484 - weight_output_loss: 1.0125 - bag_output_loss: 0.9305 - footwear_output_loss: 1.0425 - pose_output_loss: 0.9277 - emotion_output_loss: 0.9080 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5668 - footwear_output_acc: 0.4455 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 9.7225 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9836 - val_age_output_loss: 1.4629 - val_weight_output_loss: 1.0050 - val_bag_output_loss: 0.9456 - val_footwear_output_loss: 1.0462 - val_pose_output_loss: 0.9287 - val_emotion_output_loss: 0.9653 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 9/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 158s 440ms/step - loss: 9.6172 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9829 - age_output_loss: 1.4433 - weight_output_loss: 1.0056 - bag_output_loss: 0.9270 - footwear_output_loss: 1.0408 - pose_output_loss: 0.9272 - emotion_output_loss: 0.9051 - gender_output_acc: 0.5624 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4041 - weight_output_acc: 0.6347 - bag_output_acc: 0.5664 - footwear_output_acc: 0.4455 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7159 - val_loss: 9.7071 - val_gender_output_loss: 0.6850 - val_image_quality_output_loss: 0.9827 - val_age_output_loss: 1.4591 - val_weight_output_loss: 0.9992 - val_bag_output_loss: 0.9431 - val_footwear_output_loss: 1.0454 - val_pose_output_loss: 0.9281 - val_emotion_output_loss: 0.9646 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 10/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 159s 442ms/step - loss: 9.5995 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9823 - age_output_loss: 1.4387 - weight_output_loss: 1.0005 - bag_output_loss: 0.9238 - footwear_output_loss: 1.0402 - pose_output_loss: 0.9260 - emotion_output_loss: 0.9029 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4043 - weight_output_acc: 0.6346 - bag_output_acc: 0.5667 - footwear_output_acc: 0.4457 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7161 - val_loss: 9.6965 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9820 - val_age_output_loss: 1.4564 - val_weight_output_loss: 0.9952 - val_bag_output_loss: 0.9412 - val_footwear_output_loss: 1.0448 - val_pose_output_loss: 0.9278 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 11/50\n",
            "Learning rate:  0.001\n",
            "360/360 [==============================] - 161s 448ms/step - loss: 9.5861 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9816 - age_output_loss: 1.4356 - weight_output_loss: 0.9957 - bag_output_loss: 0.9215 - footwear_output_loss: 1.0393 - pose_output_loss: 0.9265 - emotion_output_loss: 0.9010 - gender_output_acc: 0.5624 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4044 - weight_output_acc: 0.6346 - bag_output_acc: 0.5666 - footwear_output_acc: 0.4457 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7161 - val_loss: 9.6888 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 1.4543 - val_weight_output_loss: 0.9922 - val_bag_output_loss: 0.9399 - val_footwear_output_loss: 1.0443 - val_pose_output_loss: 0.9276 - val_emotion_output_loss: 0.9644 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 12/50\n",
            "Learning rate:  0.0001\n",
            "359/360 [============================>.] - ETA: 0s - loss: 9.5838 - gender_output_loss: 0.6852 - image_quality_output_loss: 0.9815 - age_output_loss: 1.4346 - weight_output_loss: 0.9948 - bag_output_loss: 0.9203 - footwear_output_loss: 1.0399 - pose_output_loss: 0.9270 - emotion_output_loss: 0.9012 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4039 - weight_output_acc: 0.6340 - bag_output_acc: 0.5668 - footwear_output_acc: 0.4452 - pose_output_acc: 0.6176 - emotion_output_acc: 0.7160Learning rate:  0.0001\n",
            "360/360 [==============================] - 165s 457ms/step - loss: 9.5818 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9811 - age_output_loss: 1.4346 - weight_output_loss: 0.9943 - bag_output_loss: 0.9204 - footwear_output_loss: 1.0398 - pose_output_loss: 0.9265 - emotion_output_loss: 0.9005 - gender_output_acc: 0.5627 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4039 - weight_output_acc: 0.6344 - bag_output_acc: 0.5668 - footwear_output_acc: 0.4454 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7163 - val_loss: 9.6882 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 1.4542 - val_weight_output_loss: 0.9920 - val_bag_output_loss: 0.9397 - val_footwear_output_loss: 1.0443 - val_pose_output_loss: 0.9276 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 13/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 167s 463ms/step - loss: 9.5798 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9811 - age_output_loss: 1.4340 - weight_output_loss: 0.9941 - bag_output_loss: 0.9199 - footwear_output_loss: 1.0393 - pose_output_loss: 0.9262 - emotion_output_loss: 0.9006 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5669 - footwear_output_acc: 0.4458 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7161 - val_loss: 9.6876 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 1.4540 - val_weight_output_loss: 0.9917 - val_bag_output_loss: 0.9396 - val_footwear_output_loss: 1.0443 - val_pose_output_loss: 0.9276 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Learning rate: Epoch 14/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 163s 453ms/step - loss: 9.5799 - gender_output_loss: 0.6852 - image_quality_output_loss: 0.9814 - age_output_loss: 1.4343 - weight_output_loss: 0.9939 - bag_output_loss: 0.9199 - footwear_output_loss: 1.0394 - pose_output_loss: 0.9261 - emotion_output_loss: 0.9005 - gender_output_acc: 0.5627 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4041 - weight_output_acc: 0.6345 - bag_output_acc: 0.5666 - footwear_output_acc: 0.4455 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7159 - val_loss: 9.6870 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 1.4538 - val_weight_output_loss: 0.9915 - val_bag_output_loss: 0.9395 - val_footwear_output_loss: 1.0442 - val_pose_output_loss: 0.9276 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 15/50\n",
            " 0.0001\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 163s 452ms/step - loss: 9.5782 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9809 - age_output_loss: 1.4338 - weight_output_loss: 0.9932 - bag_output_loss: 0.9200 - footwear_output_loss: 1.0387 - pose_output_loss: 0.9269 - emotion_output_loss: 0.9001 - gender_output_acc: 0.5624 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4042 - weight_output_acc: 0.6346 - bag_output_acc: 0.5665 - footwear_output_acc: 0.4457 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7163 - val_loss: 9.6864 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 1.4536 - val_weight_output_loss: 0.9913 - val_bag_output_loss: 0.9394 - val_footwear_output_loss: 1.0442 - val_pose_output_loss: 0.9276 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Learning rate:  0.0001\n",
            "\n",
            "Epoch 16/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 156s 434ms/step - loss: 9.5782 - gender_output_loss: 0.6852 - image_quality_output_loss: 0.9808 - age_output_loss: 1.4346 - weight_output_loss: 0.9930 - bag_output_loss: 0.9196 - footwear_output_loss: 1.0388 - pose_output_loss: 0.9262 - emotion_output_loss: 0.9007 - gender_output_acc: 0.5627 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5666 - footwear_output_acc: 0.4457 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7160 - val_loss: 9.6858 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 1.4535 - val_weight_output_loss: 0.9910 - val_bag_output_loss: 0.9393 - val_footwear_output_loss: 1.0442 - val_pose_output_loss: 0.9276 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 17/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 159s 442ms/step - loss: 9.5786 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9810 - age_output_loss: 1.4332 - weight_output_loss: 0.9931 - bag_output_loss: 0.9199 - footwear_output_loss: 1.0393 - pose_output_loss: 0.9266 - emotion_output_loss: 0.9010 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5666 - footwear_output_acc: 0.4459 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7159 - val_loss: 9.6852 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 1.4533 - val_weight_output_loss: 0.9908 - val_bag_output_loss: 0.9392 - val_footwear_output_loss: 1.0441 - val_pose_output_loss: 0.9276 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 18/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 159s 443ms/step - loss: 9.5769 - gender_output_loss: 0.6851 - image_quality_output_loss: 0.9809 - age_output_loss: 1.4336 - weight_output_loss: 0.9935 - bag_output_loss: 0.9193 - footwear_output_loss: 1.0392 - pose_output_loss: 0.9262 - emotion_output_loss: 0.8999 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4042 - weight_output_acc: 0.6342 - bag_output_acc: 0.5668 - footwear_output_acc: 0.4457 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7162 - val_loss: 9.6847 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 1.4532 - val_weight_output_loss: 0.9906 - val_bag_output_loss: 0.9391 - val_footwear_output_loss: 1.0441 - val_pose_output_loss: 0.9276 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 19/50\n",
            "Learning rate:  0.0001\n",
            "359/360 [============================>.] - ETA: 0s - loss: 9.5762 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9808 - age_output_loss: 1.4332 - weight_output_loss: 0.9924 - bag_output_loss: 0.9194 - footwear_output_loss: 1.0389 - pose_output_loss: 0.9268 - emotion_output_loss: 0.9003 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5534 - age_output_acc: 0.4040 - weight_output_acc: 0.6347 - bag_output_acc: 0.5669 - footwear_output_acc: 0.4451 - pose_output_acc: 0.6176 - emotion_output_acc: 0.7161\n",
            "\n",
            "360/360 [==============================] - 159s 442ms/step - loss: 9.5758 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9810 - age_output_loss: 1.4332 - weight_output_loss: 0.9926 - bag_output_loss: 0.9192 - footwear_output_loss: 1.0387 - pose_output_loss: 0.9264 - emotion_output_loss: 0.9002 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4043 - weight_output_acc: 0.6347 - bag_output_acc: 0.5670 - footwear_output_acc: 0.4454 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7161 - val_loss: 9.6841 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9813 - val_age_output_loss: 1.4530 - val_weight_output_loss: 0.9904 - val_bag_output_loss: 0.9390 - val_footwear_output_loss: 1.0441 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 20/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 158s 438ms/step - loss: 9.5747 - gender_output_loss: 0.6855 - image_quality_output_loss: 0.9811 - age_output_loss: 1.4326 - weight_output_loss: 0.9923 - bag_output_loss: 0.9191 - footwear_output_loss: 1.0390 - pose_output_loss: 0.9261 - emotion_output_loss: 0.8999 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6349 - bag_output_acc: 0.5665 - footwear_output_acc: 0.4462 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7161 - val_loss: 9.6836 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9813 - val_age_output_loss: 1.4529 - val_weight_output_loss: 0.9902 - val_bag_output_loss: 0.9389 - val_footwear_output_loss: 1.0440 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 21/50\n",
            "Learning rate:  0.0001\n",
            "360/360 [==============================] - 154s 428ms/step - loss: 9.5732 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9809 - age_output_loss: 1.4325 - weight_output_loss: 0.9915 - bag_output_loss: 0.9192 - footwear_output_loss: 1.0386 - pose_output_loss: 0.9265 - emotion_output_loss: 0.8996 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6347 - bag_output_acc: 0.5667 - footwear_output_acc: 0.4457 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7164 - val_loss: 9.6831 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9813 - val_age_output_loss: 1.4527 - val_weight_output_loss: 0.9900 - val_bag_output_loss: 0.9388 - val_footwear_output_loss: 1.0440 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 22/50\n",
            "Learning rate:  5e-05\n",
            "360/360 [==============================] - 158s 438ms/step - loss: 9.5747 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9810 - age_output_loss: 1.4327 - weight_output_loss: 0.9916 - bag_output_loss: 0.9188 - footwear_output_loss: 1.0393 - pose_output_loss: 0.9269 - emotion_output_loss: 0.8998 - gender_output_acc: 0.5624 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4041 - weight_output_acc: 0.6346 - bag_output_acc: 0.5666 - footwear_output_acc: 0.4456 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7161 - val_loss: 9.6829 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9813 - val_age_output_loss: 1.4527 - val_weight_output_loss: 0.9899 - val_bag_output_loss: 0.9388 - val_footwear_output_loss: 1.0440 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 23/50\n",
            "Learning rate:  5e-05\n",
            "359/360 [============================>.] - ETA: 0s - loss: 9.5747 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9811 - age_output_loss: 1.4329 - weight_output_loss: 0.9918 - bag_output_loss: 0.9189 - footwear_output_loss: 1.0392 - pose_output_loss: 0.9261 - emotion_output_loss: 0.9003 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5525 - age_output_acc: 0.4041 - weight_output_acc: 0.6347 - bag_output_acc: 0.5667 - footwear_output_acc: 0.4455 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7160\n",
            "Learning rate:  5e-05\n",
            "360/360 [==============================] - 159s 442ms/step - loss: 9.5739 - gender_output_loss: 0.6852 - image_quality_output_loss: 0.9808 - age_output_loss: 1.4329 - weight_output_loss: 0.9920 - bag_output_loss: 0.9188 - footwear_output_loss: 1.0392 - pose_output_loss: 0.9262 - emotion_output_loss: 0.8998 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4042 - weight_output_acc: 0.6345 - bag_output_acc: 0.5667 - footwear_output_acc: 0.4457 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7162 - val_loss: 9.6827 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9813 - val_age_output_loss: 1.4526 - val_weight_output_loss: 0.9899 - val_bag_output_loss: 0.9388 - val_footwear_output_loss: 1.0440 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 24/50\n",
            "Learning rate:  5e-05\n",
            "360/360 [==============================] - 158s 438ms/step - loss: 9.5731 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9808 - age_output_loss: 1.4321 - weight_output_loss: 0.9917 - bag_output_loss: 0.9189 - footwear_output_loss: 1.0390 - pose_output_loss: 0.9263 - emotion_output_loss: 0.8999 - gender_output_acc: 0.5627 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6347 - bag_output_acc: 0.5666 - footwear_output_acc: 0.4466 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 9.6825 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9813 - val_age_output_loss: 1.4526 - val_weight_output_loss: 0.9898 - val_bag_output_loss: 0.9387 - val_footwear_output_loss: 1.0440 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 25/50\n",
            "Learning rate:  5e-05\n",
            "Learning rate:  5e-05\n",
            "359/360 [============================>.] - ETA: 0s - loss: 9.5726 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9813 - age_output_loss: 1.4320 - weight_output_loss: 0.9918 - bag_output_loss: 0.9179 - footwear_output_loss: 1.0389 - pose_output_loss: 0.9264 - emotion_output_loss: 0.8999 - gender_output_acc: 0.5629 - image_quality_output_acc: 0.5526 - age_output_acc: 0.4046 - weight_output_acc: 0.6345 - bag_output_acc: 0.5670 - footwear_output_acc: 0.4465 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7160 5e-05\n",
            "360/360 [==============================] - 159s 440ms/step - loss: 9.5725 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9812 - age_output_loss: 1.4322 - weight_output_loss: 0.9920 - bag_output_loss: 0.9184 - footwear_output_loss: 1.0388 - pose_output_loss: 0.9264 - emotion_output_loss: 0.8991 - gender_output_acc: 0.5627 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4043 - weight_output_acc: 0.6344 - bag_output_acc: 0.5667 - footwear_output_acc: 0.4463 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7163 - val_loss: 9.6822 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4525 - val_weight_output_loss: 0.9897 - val_bag_output_loss: 0.9387 - val_footwear_output_loss: 1.0440 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 26/50\n",
            "Learning rate:  5e-05\n",
            "360/360 [==============================] - 153s 425ms/step - loss: 9.5735 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9808 - age_output_loss: 1.4326 - weight_output_loss: 0.9912 - bag_output_loss: 0.9185 - footwear_output_loss: 1.0393 - pose_output_loss: 0.9263 - emotion_output_loss: 0.9003 - gender_output_acc: 0.5624 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4042 - weight_output_acc: 0.6349 - bag_output_acc: 0.5667 - footwear_output_acc: 0.4458 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7159 - val_loss: 9.6820 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4524 - val_weight_output_loss: 0.9896 - val_bag_output_loss: 0.9386 - val_footwear_output_loss: 1.0440 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 27/50\n",
            "Learning rate:  5e-05\n",
            "359/360 [============================>.] - ETA: 0s - loss: 9.5717 - gender_output_loss: 0.6851 - image_quality_output_loss: 0.9806 - age_output_loss: 1.4325 - weight_output_loss: 0.9914 - bag_output_loss: 0.9187 - footwear_output_loss: 1.0390 - pose_output_loss: 0.9263 - emotion_output_loss: 0.8991 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4040 - weight_output_acc: 0.6344 - bag_output_acc: 0.5663 - footwear_output_acc: 0.4458 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7166 5e-05\n",
            "360/360 [==============================] - 158s 438ms/step - loss: 9.5713 - gender_output_loss: 0.6852 - image_quality_output_loss: 0.9806 - age_output_loss: 1.4320 - weight_output_loss: 0.9909 - bag_output_loss: 0.9185 - footwear_output_loss: 1.0390 - pose_output_loss: 0.9265 - emotion_output_loss: 0.8995 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4043 - weight_output_acc: 0.6347 - bag_output_acc: 0.5666 - footwear_output_acc: 0.4462 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7164 - val_loss: 9.6817 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4523 - val_weight_output_loss: 0.9895 - val_bag_output_loss: 0.9386 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 28/50\n",
            "Learning rate:  5e-05\n",
            "360/360 [==============================] - 160s 445ms/step - loss: 9.5715 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9812 - age_output_loss: 1.4322 - weight_output_loss: 0.9901 - bag_output_loss: 0.9188 - footwear_output_loss: 1.0393 - pose_output_loss: 0.9261 - emotion_output_loss: 0.8994 - gender_output_acc: 0.5624 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4043 - weight_output_acc: 0.6348 - bag_output_acc: 0.5667 - footwear_output_acc: 0.4457 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7165 - val_loss: 9.6815 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4523 - val_weight_output_loss: 0.9894 - val_bag_output_loss: 0.9386 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 29/50\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Learning rate:  5e-05\n",
            "359/360 [============================>.] - ETA: 0s - loss: 9.5699 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9810 - age_output_loss: 1.4314 - weight_output_loss: 0.9910 - bag_output_loss: 0.9184 - footwear_output_loss: 1.0381 - pose_output_loss: 0.9250 - emotion_output_loss: 0.9007 - gender_output_acc: 0.5620 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4044 - weight_output_acc: 0.6346 - bag_output_acc: 0.5667 - footwear_output_acc: 0.4458 - pose_output_acc: 0.6191 - emotion_output_acc: 0.7158\n",
            "360/360 [==============================] - 158s 439ms/step - loss: 9.5700 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9810 - age_output_loss: 1.4316 - weight_output_loss: 0.9907 - bag_output_loss: 0.9186 - footwear_output_loss: 1.0382 - pose_output_loss: 0.9257 - emotion_output_loss: 0.8999 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4042 - weight_output_acc: 0.6348 - bag_output_acc: 0.5666 - footwear_output_acc: 0.4458 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7161 - val_loss: 9.6813 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4522 - val_weight_output_loss: 0.9893 - val_bag_output_loss: 0.9385 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 30/50\n",
            "Learning rate:  5e-05\n",
            "359/360 [============================>.] - ETA: 0s - loss: 9.5705 - gender_output_loss: 0.6855 - image_quality_output_loss: 0.9809 - age_output_loss: 1.4314 - weight_output_loss: 0.9911 - bag_output_loss: 0.9185 - footwear_output_loss: 1.0386 - pose_output_loss: 0.9255 - emotion_output_loss: 0.9000 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5526 - age_output_acc: 0.4043 - weight_output_acc: 0.6347 - bag_output_acc: 0.5662 - footwear_output_acc: 0.4458 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7161Epoch 30/50\n",
            "360/360 [==============================] - 161s 446ms/step - loss: 9.5707 - gender_output_loss: 0.6855 - image_quality_output_loss: 0.9806 - age_output_loss: 1.4316 - weight_output_loss: 0.9908 - bag_output_loss: 0.9188 - footwear_output_loss: 1.0386 - pose_output_loss: 0.9265 - emotion_output_loss: 0.8993 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4043 - weight_output_acc: 0.6348 - bag_output_acc: 0.5663 - footwear_output_acc: 0.4461 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7165 - val_loss: 9.6811 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4522 - val_weight_output_loss: 0.9892 - val_bag_output_loss: 0.9385 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 31/50\n",
            "Learning rate:  5e-05\n",
            "360/360 [==============================] - 155s 429ms/step - loss: 9.5725 - gender_output_loss: 0.6852 - image_quality_output_loss: 0.9811 - age_output_loss: 1.4324 - weight_output_loss: 0.9907 - bag_output_loss: 0.9186 - footwear_output_loss: 1.0387 - pose_output_loss: 0.9263 - emotion_output_loss: 0.9003 - gender_output_acc: 0.5629 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4041 - weight_output_acc: 0.6345 - bag_output_acc: 0.5665 - footwear_output_acc: 0.4457 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7160 - val_loss: 9.6809 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4521 - val_weight_output_loss: 0.9892 - val_bag_output_loss: 0.9384 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 32/50\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 156s 434ms/step - loss: 9.5705 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9808 - age_output_loss: 1.4316 - weight_output_loss: 0.9909 - bag_output_loss: 0.9181 - footwear_output_loss: 1.0385 - pose_output_loss: 0.9264 - emotion_output_loss: 0.8999 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4045 - weight_output_acc: 0.6346 - bag_output_acc: 0.5668 - footwear_output_acc: 0.4460 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 9.6808 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4521 - val_weight_output_loss: 0.9891 - val_bag_output_loss: 0.9384 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 33/50\n",
            "Learning rate:  1e-05\n",
            "359/360 [============================>.] - ETA: 0s - loss: 9.5718 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9812 - age_output_loss: 1.4320 - weight_output_loss: 0.9907 - bag_output_loss: 0.9191 - footwear_output_loss: 1.0384 - pose_output_loss: 0.9259 - emotion_output_loss: 0.9003 - gender_output_acc: 0.5627 - image_quality_output_acc: 0.5527 - age_output_acc: 0.4042 - weight_output_acc: 0.6345 - bag_output_acc: 0.5662 - footwear_output_acc: 0.4463 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7161Learning rate:  1e-05\n",
            "360/360 [==============================] - 161s 447ms/step - loss: 9.5718 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9809 - age_output_loss: 1.4320 - weight_output_loss: 0.9906 - bag_output_loss: 0.9188 - footwear_output_loss: 1.0385 - pose_output_loss: 0.9261 - emotion_output_loss: 0.9005 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4039 - weight_output_acc: 0.6345 - bag_output_acc: 0.5665 - footwear_output_acc: 0.4463 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7160 - val_loss: 9.6808 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4521 - val_weight_output_loss: 0.9891 - val_bag_output_loss: 0.9384 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 34/50\n",
            "Learning rate:  1e-05\n",
            "359/360 [============================>.] - ETA: 0s - loss: 9.5705 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9802 - age_output_loss: 1.4323 - weight_output_loss: 0.9904 - bag_output_loss: 0.9181 - footwear_output_loss: 1.0396 - pose_output_loss: 0.9258 - emotion_output_loss: 0.8996 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4039 - weight_output_acc: 0.6347 - bag_output_acc: 0.5667 - footwear_output_acc: 0.4463 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7162 1e-05\n",
            "360/360 [==============================] - 163s 452ms/step - loss: 9.5713 - gender_output_loss: 0.6855 - image_quality_output_loss: 0.9806 - age_output_loss: 1.4321 - weight_output_loss: 0.9905 - bag_output_loss: 0.9180 - footwear_output_loss: 1.0396 - pose_output_loss: 0.9262 - emotion_output_loss: 0.8999 - gender_output_acc: 0.5627 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4043 - weight_output_acc: 0.6347 - bag_output_acc: 0.5668 - footwear_output_acc: 0.4463 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 9.6807 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4521 - val_weight_output_loss: 0.9891 - val_bag_output_loss: 0.9384 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 35/50\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 161s 448ms/step - loss: 9.5705 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9808 - age_output_loss: 1.4320 - weight_output_loss: 0.9905 - bag_output_loss: 0.9179 - footwear_output_loss: 1.0387 - pose_output_loss: 0.9262 - emotion_output_loss: 0.9000 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4043 - weight_output_acc: 0.6347 - bag_output_acc: 0.5667 - footwear_output_acc: 0.4465 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 9.6807 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4521 - val_weight_output_loss: 0.9891 - val_bag_output_loss: 0.9384 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 36/50\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 161s 448ms/step - loss: 9.5702 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9810 - age_output_loss: 1.4315 - weight_output_loss: 0.9904 - bag_output_loss: 0.9184 - footwear_output_loss: 1.0389 - pose_output_loss: 0.9260 - emotion_output_loss: 0.8998 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4040 - weight_output_acc: 0.6347 - bag_output_acc: 0.5668 - footwear_output_acc: 0.4460 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 9.6807 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4520 - val_weight_output_loss: 0.9891 - val_bag_output_loss: 0.9384 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 37/50\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 167s 465ms/step - loss: 9.5710 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9809 - age_output_loss: 1.4317 - weight_output_loss: 0.9913 - bag_output_loss: 0.9186 - footwear_output_loss: 1.0391 - pose_output_loss: 0.9259 - emotion_output_loss: 0.8992 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6343 - bag_output_acc: 0.5664 - footwear_output_acc: 0.4465 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7161 - val_loss: 9.6806 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4520 - val_weight_output_loss: 0.9891 - val_bag_output_loss: 0.9384 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 38/50\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 170s 471ms/step - loss: 9.5701 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9810 - age_output_loss: 1.4318 - weight_output_loss: 0.9906 - bag_output_loss: 0.9181 - footwear_output_loss: 1.0385 - pose_output_loss: 0.9263 - emotion_output_loss: 0.8994 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4045 - weight_output_acc: 0.6348 - bag_output_acc: 0.5663 - footwear_output_acc: 0.4463 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7162 - val_loss: 9.6806 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4520 - val_weight_output_loss: 0.9890 - val_bag_output_loss: 0.9384 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 39/50\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 168s 467ms/step - loss: 9.5712 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9811 - age_output_loss: 1.4313 - weight_output_loss: 0.9905 - bag_output_loss: 0.9184 - footwear_output_loss: 1.0390 - pose_output_loss: 0.9265 - emotion_output_loss: 0.8998 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4043 - weight_output_acc: 0.6348 - bag_output_acc: 0.5665 - footwear_output_acc: 0.4460 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7161 - val_loss: 9.6806 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4520 - val_weight_output_loss: 0.9890 - val_bag_output_loss: 0.9384 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 39/50\n",
            "Epoch 40/50\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 171s 475ms/step - loss: 9.5715 - gender_output_loss: 0.6855 - image_quality_output_loss: 0.9808 - age_output_loss: 1.4320 - weight_output_loss: 0.9911 - bag_output_loss: 0.9183 - footwear_output_loss: 1.0391 - pose_output_loss: 0.9262 - emotion_output_loss: 0.8996 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4040 - weight_output_acc: 0.6346 - bag_output_acc: 0.5664 - footwear_output_acc: 0.4458 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7161 - val_loss: 9.6805 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9811 - val_age_output_loss: 1.4520 - val_weight_output_loss: 0.9890 - val_bag_output_loss: 0.9384 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 41/50\n",
            "Learning rate:  1e-05\n",
            "360/360 [==============================] - 166s 462ms/step - loss: 9.5693 - gender_output_loss: 0.6851 - image_quality_output_loss: 0.9809 - age_output_loss: 1.4312 - weight_output_loss: 0.9909 - bag_output_loss: 0.9184 - footwear_output_loss: 1.0382 - pose_output_loss: 0.9262 - emotion_output_loss: 0.8994 - gender_output_acc: 0.5626 - image_quality_output_acc: 0.5526 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5668 - footwear_output_acc: 0.4465 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7162 - val_loss: 9.6804 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9811 - val_age_output_loss: 1.4520 - val_weight_output_loss: 0.9890 - val_bag_output_loss: 0.9384 - val_footwear_output_loss: 1.0438 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9646 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 42/50\n",
            "Learning rate:  5e-06\n",
            "359/360 [============================>.] - ETA: 0s - loss: 9.5708 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9806 - age_output_loss: 1.4322 - weight_output_loss: 0.9901 - bag_output_loss: 0.9183 - footwear_output_loss: 1.0386 - pose_output_loss: 0.9269 - emotion_output_loss: 0.8998 - gender_output_acc: 0.5627 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4041 - weight_output_acc: 0.6347 - bag_output_acc: 0.5666 - footwear_output_acc: 0.4462 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7160Epoch 42/50\n",
            "360/360 [==============================] - 171s 474ms/step - loss: 9.5700 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9807 - age_output_loss: 1.4319 - weight_output_loss: 0.9904 - bag_output_loss: 0.9181 - footwear_output_loss: 1.0384 - pose_output_loss: 0.9265 - emotion_output_loss: 0.8999 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5668 - footwear_output_acc: 0.4465 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7160 - val_loss: 9.6804 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9811 - val_age_output_loss: 1.4520 - val_weight_output_loss: 0.9890 - val_bag_output_loss: 0.9384 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 43/50\n",
            "Learning rate:  5e-06\n",
            "360/360 [==============================] - 170s 472ms/step - loss: 9.5712 - gender_output_loss: 0.6855 - image_quality_output_loss: 0.9814 - age_output_loss: 1.4318 - weight_output_loss: 0.9906 - bag_output_loss: 0.9185 - footwear_output_loss: 1.0384 - pose_output_loss: 0.9265 - emotion_output_loss: 0.8994 - gender_output_acc: 0.5624 - image_quality_output_acc: 0.5526 - age_output_acc: 0.4043 - weight_output_acc: 0.6346 - bag_output_acc: 0.5666 - footwear_output_acc: 0.4464 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7163 - val_loss: 9.6804 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4520 - val_weight_output_loss: 0.9890 - val_bag_output_loss: 0.9384 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 44/50\n",
            "Learning rate:  5e-06\n",
            "360/360 [==============================] - 168s 466ms/step - loss: 9.5694 - gender_output_loss: 0.6851 - image_quality_output_loss: 0.9805 - age_output_loss: 1.4314 - weight_output_loss: 0.9911 - bag_output_loss: 0.9180 - footwear_output_loss: 1.0382 - pose_output_loss: 0.9261 - emotion_output_loss: 0.9000 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4043 - weight_output_acc: 0.6344 - bag_output_acc: 0.5667 - footwear_output_acc: 0.4463 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7159 - val_loss: 9.6804 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9811 - val_age_output_loss: 1.4520 - val_weight_output_loss: 0.9890 - val_bag_output_loss: 0.9384 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 45/50\n",
            "Learning rate:  5e-06\n",
            "360/360 [==============================] - 172s 479ms/step - loss: 9.5709 - gender_output_loss: 0.6856 - image_quality_output_loss: 0.9811 - age_output_loss: 1.4326 - weight_output_loss: 0.9906 - bag_output_loss: 0.9184 - footwear_output_loss: 1.0386 - pose_output_loss: 0.9260 - emotion_output_loss: 0.8990 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4042 - weight_output_acc: 0.6346 - bag_output_acc: 0.5663 - footwear_output_acc: 0.4458 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7163 - val_loss: 9.6804 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4520 - val_weight_output_loss: 0.9890 - val_bag_output_loss: 0.9384 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 46/50\n",
            "Learning rate:  5e-06\n",
            "360/360 [==============================] - 163s 452ms/step - loss: 9.5683 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9805 - age_output_loss: 1.4314 - weight_output_loss: 0.9905 - bag_output_loss: 0.9177 - footwear_output_loss: 1.0388 - pose_output_loss: 0.9263 - emotion_output_loss: 0.8987 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4041 - weight_output_acc: 0.6346 - bag_output_acc: 0.5668 - footwear_output_acc: 0.4462 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7164 - val_loss: 9.6804 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4520 - val_weight_output_loss: 0.9890 - val_bag_output_loss: 0.9383 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 47/50\n",
            "Learning rate:  5e-06\n",
            "360/360 [==============================] - 169s 469ms/step - loss: 9.5691 - gender_output_loss: 0.6852 - image_quality_output_loss: 0.9808 - age_output_loss: 1.4316 - weight_output_loss: 0.9893 - bag_output_loss: 0.9182 - footwear_output_loss: 1.0387 - pose_output_loss: 0.9266 - emotion_output_loss: 0.8998 - gender_output_acc: 0.5628 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4043 - weight_output_acc: 0.6349 - bag_output_acc: 0.5665 - footwear_output_acc: 0.4461 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 9.6804 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4520 - val_weight_output_loss: 0.9889 - val_bag_output_loss: 0.9383 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 48/50\n",
            "Learning rate:  5e-06\n",
            "360/360 [==============================] - 173s 481ms/step - loss: 9.5705 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9804 - age_output_loss: 1.4319 - weight_output_loss: 0.9909 - bag_output_loss: 0.9182 - footwear_output_loss: 1.0383 - pose_output_loss: 0.9262 - emotion_output_loss: 0.9003 - gender_output_acc: 0.5627 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4038 - weight_output_acc: 0.6344 - bag_output_acc: 0.5665 - footwear_output_acc: 0.4463 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7158 - val_loss: 9.6804 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9812 - val_age_output_loss: 1.4520 - val_weight_output_loss: 0.9889 - val_bag_output_loss: 0.9383 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 49/50\n",
            "Learning rate:  5e-06\n",
            "360/360 [==============================] - 171s 474ms/step - loss: 9.5695 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9805 - age_output_loss: 1.4318 - weight_output_loss: 0.9905 - bag_output_loss: 0.9182 - footwear_output_loss: 1.0384 - pose_output_loss: 0.9262 - emotion_output_loss: 0.8994 - gender_output_acc: 0.5627 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4040 - weight_output_acc: 0.6348 - bag_output_acc: 0.5666 - footwear_output_acc: 0.4460 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7164 - val_loss: 9.6803 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9811 - val_age_output_loss: 1.4519 - val_weight_output_loss: 0.9889 - val_bag_output_loss: 0.9383 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "Epoch 50/50\n",
            "Learning rate:  5e-06\n",
            "359/360 [============================>.] - ETA: 0s - loss: 9.5691 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9812 - age_output_loss: 1.4305 - weight_output_loss: 0.9904 - bag_output_loss: 0.9182 - footwear_output_loss: 1.0393 - pose_output_loss: 0.9263 - emotion_output_loss: 0.8989 - gender_output_acc: 0.5624 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4047 - weight_output_acc: 0.6350 - bag_output_acc: 0.5666 - footwear_output_acc: 0.4459 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7163\n",
            "Learning rate:  5e-06\n",
            "360/360 [==============================] - 172s 477ms/step - loss: 9.5693 - gender_output_loss: 0.6853 - image_quality_output_loss: 0.9812 - age_output_loss: 1.4308 - weight_output_loss: 0.9905 - bag_output_loss: 0.9183 - footwear_output_loss: 1.0391 - pose_output_loss: 0.9259 - emotion_output_loss: 0.8991 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4044 - weight_output_acc: 0.6348 - bag_output_acc: 0.5664 - footwear_output_acc: 0.4460 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7162 - val_loss: 9.6803 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9811 - val_age_output_loss: 1.4519 - val_weight_output_loss: 0.9889 - val_bag_output_loss: 0.9383 - val_footwear_output_loss: 1.0439 - val_pose_output_loss: 0.9275 - val_emotion_output_loss: 0.9645 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}