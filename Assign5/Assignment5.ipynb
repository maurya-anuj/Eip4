{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Final_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maurya-anuj/Eip4/blob/master/Assign5/Assignment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxG3tULh7LYA",
        "colab_type": "code",
        "outputId": "e2174418-7c6f-4e9b-cf08-e4aac1c61777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/Deep Vision/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX9N9bFi7fP1",
        "colab_type": "code",
        "outputId": "e73a9862-4353-4554-c1e2-3d8b05f1f18b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "# import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# from functools import partial\n",
        "# from pathlib import Path \n",
        "# from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "# from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "# from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "# from keras.optimizers import SGD\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "batch = 64\n",
        "\n",
        "#########################\n",
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()\n",
        "\n",
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "\n",
        "##########  AUGUMENTATION GEN.... CUTOUT  #####################\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augumentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augumentation = augumentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        # image = image/255\n",
        "        if self.augumentation is not None:\n",
        "            image = self.augumentation.flow(image, shuffle=False, batch_size=batch).next()\n",
        "        image = image/255\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15, random_state=1)\n",
        "print(train_df.shape, val_df.shape)\n",
        "\n",
        "########################################################\n",
        "pixel_level = False\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.1, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser\n",
        "\n",
        "########################################\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "aug = ImageDataGenerator(\n",
        "        # # set input mean to 0 over the dataset\n",
        "        # featurewise_center=False,\n",
        "        # # set each sample mean to 0\n",
        "        # samplewise_center=False,\n",
        "        # # divide inputs by std of dataset\n",
        "        # featurewise_std_normalization=False,\n",
        "        # # divide each input by its std\n",
        "        # samplewise_std_normalization=False,\n",
        "        # # apply ZCA whitening\n",
        "        # zca_whitening=False,\n",
        "        # # epsilon for ZCA whitening\n",
        "        # zca_epsilon=1e-06,\n",
        "        # # randomly rotate images in the range (deg 0 to 180)\n",
        "        # rotation_range=10,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # # set range for random shear\n",
        "        # shear_range=0.,\n",
        "        # # set range for random zoom\n",
        "        # zoom_range=0.1,\n",
        "        # # set range for random channel shifts\n",
        "        # channel_shift_range=0.,\n",
        "        # # set mode for filling points outside the input boundaries\n",
        "        # fill_mode='nearest',\n",
        "        # # value used for fill_mode = \"constant\"\n",
        "        # cval=0.,\n",
        "        # # randomly flip images\n",
        "        # horizontal_flip=True,\n",
        "        # # randomly flip images\n",
        "        # vertical_flip=False,\n",
        "        # # set rescaling factor (applied before any other transformation)\n",
        "        # rescale=1.0/255.0,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=pixel_level),\n",
        "        # # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        # data_format=None,\n",
        "        # # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=batch, augumentation = aug)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=batch*2, shuffle=False, augumentation = None)  # val_df\n",
        "\n",
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "\n",
        "#########################################################################################\n",
        "from keras.models import model_from_json\n",
        "json_file = open(\"/content/gdrive/My Drive/Deep Vision/resnet34_new.json\", 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model = model_from_json(loaded_model_json)\n",
        "model.layers.pop() # Remove last dense layer\n",
        "\n",
        "neck = model.output\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.1)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"sigmoid\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "model = Model(\n",
        "    inputs=model.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")\n",
        "\n",
        "####################  COMPILE  ############################\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "import os\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "model_save = keras.callbacks.ModelCheckpoint('model{epoch:08d}.h5', period=5) \n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=.05,\n",
        "                               monitor='val_loss',\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               verbose=1,\n",
        "                               min_delta=0.01,\n",
        "                               min_lr=0.5e-7)\n",
        "def lr_schedule(epoch):\n",
        "    lr = 1.4\n",
        "    if epoch > 60:\n",
        "        lr *= 1e-1\n",
        "    elif epoch > 30:\n",
        "        lr *= 1e-1\n",
        "    elif epoch > 25:\n",
        "        lr *= 1e-1\n",
        "    elif epoch > 15:\n",
        "        lr *= 0.5e-1\n",
        "    elif epoch > 10:\n",
        "        lr *= 1e-1\n",
        "    elif epoch > 8:\n",
        "        lr *= 0.5\n",
        "    elif epoch > 5:\n",
        "        lr = 0.9\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "callbacks = [lr_scheduler, model_save]\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "opt = SGD(lr=0.1)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    # loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "######################  FIT  #######################################\n",
        "model_info = model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    callbacks = callbacks\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(11537, 28) (2036, 28)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/100\n",
            "Learning rate:  1.4\n",
            "180/180 [==============================] - 165s 917ms/step - loss: 9.4844 - gender_output_loss: 0.6834 - image_quality_output_loss: 0.9824 - age_output_loss: 1.4297 - weight_output_loss: 0.9927 - bag_output_loss: 0.9177 - footwear_output_loss: 1.0138 - pose_output_loss: 0.9324 - emotion_output_loss: 0.9120 - gender_output_acc: 0.5604 - image_quality_output_acc: 0.5520 - age_output_acc: 0.4038 - weight_output_acc: 0.6322 - bag_output_acc: 0.5657 - footwear_output_acc: 0.4851 - pose_output_acc: 0.6174 - emotion_output_acc: 0.7126 - val_loss: 9.6163 - val_gender_output_loss: 0.6903 - val_image_quality_output_loss: 0.9960 - val_age_output_loss: 1.4621 - val_weight_output_loss: 0.9982 - val_bag_output_loss: 0.9398 - val_footwear_output_loss: 1.0587 - val_pose_output_loss: 0.9407 - val_emotion_output_loss: 0.9900 - val_gender_output_acc: 0.5646 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5505 - val_footwear_output_acc: 0.4573 - val_pose_output_acc: 0.6156 - val_emotion_output_acc: 0.6880\n",
            "Epoch 2/100\n",
            "Learning rate:  1.4\n",
            "180/180 [==============================] - 144s 797ms/step - loss: 9.2566 - gender_output_loss: 0.6784 - image_quality_output_loss: 0.9820 - age_output_loss: 1.4219 - weight_output_loss: 0.9851 - bag_output_loss: 0.9125 - footwear_output_loss: 0.9824 - pose_output_loss: 0.9283 - emotion_output_loss: 0.8993 - gender_output_acc: 0.5732 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4045 - weight_output_acc: 0.6345 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5238 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7162 - val_loss: 9.3230 - val_gender_output_loss: 0.6904 - val_image_quality_output_loss: 0.9879 - val_age_output_loss: 1.4413 - val_weight_output_loss: 0.9861 - val_bag_output_loss: 0.9380 - val_footwear_output_loss: 0.9949 - val_pose_output_loss: 0.9301 - val_emotion_output_loss: 0.9595 - val_gender_output_acc: 0.5646 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5505 - val_footwear_output_acc: 0.5052 - val_pose_output_acc: 0.6156 - val_emotion_output_acc: 0.6880\n",
            "Epoch 3/100\n",
            "Learning rate:  1.4\n",
            "180/180 [==============================] - 143s 794ms/step - loss: 9.0830 - gender_output_loss: 0.6731 - image_quality_output_loss: 0.9799 - age_output_loss: 1.4171 - weight_output_loss: 0.9830 - bag_output_loss: 0.9080 - footwear_output_loss: 0.9661 - pose_output_loss: 0.9275 - emotion_output_loss: 0.8995 - gender_output_acc: 0.5772 - image_quality_output_acc: 0.5534 - age_output_acc: 0.4045 - weight_output_acc: 0.6344 - bag_output_acc: 0.5663 - footwear_output_acc: 0.5367 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7162 - val_loss: 9.2742 - val_gender_output_loss: 0.6698 - val_image_quality_output_loss: 0.9890 - val_age_output_loss: 1.4502 - val_weight_output_loss: 0.9931 - val_bag_output_loss: 0.9512 - val_footwear_output_loss: 1.0744 - val_pose_output_loss: 0.9268 - val_emotion_output_loss: 0.9556 - val_gender_output_acc: 0.6031 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5505 - val_footwear_output_acc: 0.3651 - val_pose_output_acc: 0.6156 - val_emotion_output_acc: 0.6880\n",
            "Epoch 4/100\n",
            "Learning rate:  1.4\n",
            "180/180 [==============================] - 142s 790ms/step - loss: 8.9222 - gender_output_loss: 0.6639 - image_quality_output_loss: 0.9802 - age_output_loss: 1.4140 - weight_output_loss: 0.9791 - bag_output_loss: 0.9048 - footwear_output_loss: 0.9521 - pose_output_loss: 0.9260 - emotion_output_loss: 0.8975 - gender_output_acc: 0.5938 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4037 - weight_output_acc: 0.6347 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5492 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7164 - val_loss: 8.9639 - val_gender_output_loss: 0.6446 - val_image_quality_output_loss: 0.9824 - val_age_output_loss: 1.4325 - val_weight_output_loss: 0.9831 - val_bag_output_loss: 0.9196 - val_footwear_output_loss: 0.9744 - val_pose_output_loss: 0.9249 - val_emotion_output_loss: 0.9562 - val_gender_output_acc: 0.6260 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5505 - val_footwear_output_acc: 0.5245 - val_pose_output_acc: 0.6156 - val_emotion_output_acc: 0.6880\n",
            "Epoch 5/100\n",
            "Learning rate:  1.4\n",
            "180/180 [==============================] - 141s 786ms/step - loss: 8.7746 - gender_output_loss: 0.6560 - image_quality_output_loss: 0.9786 - age_output_loss: 1.4120 - weight_output_loss: 0.9793 - bag_output_loss: 0.9026 - footwear_output_loss: 0.9322 - pose_output_loss: 0.9240 - emotion_output_loss: 0.8974 - gender_output_acc: 0.5984 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4042 - weight_output_acc: 0.6346 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5584 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 9.1960 - val_gender_output_loss: 0.6952 - val_image_quality_output_loss: 0.9770 - val_age_output_loss: 1.4535 - val_weight_output_loss: 0.9910 - val_bag_output_loss: 0.9406 - val_footwear_output_loss: 1.2101 - val_pose_output_loss: 0.9306 - val_emotion_output_loss: 0.9577 - val_gender_output_acc: 0.4547 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5505 - val_footwear_output_acc: 0.3635 - val_pose_output_acc: 0.6156 - val_emotion_output_acc: 0.6880\n",
            "Epoch 6/100\n",
            "Learning rate:  1.4\n",
            "180/180 [==============================] - 135s 750ms/step - loss: 8.6517 - gender_output_loss: 0.6564 - image_quality_output_loss: 0.9788 - age_output_loss: 1.4090 - weight_output_loss: 0.9787 - bag_output_loss: 0.9000 - footwear_output_loss: 0.9213 - pose_output_loss: 0.9189 - emotion_output_loss: 0.8961 - gender_output_acc: 0.6030 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4042 - weight_output_acc: 0.6348 - bag_output_acc: 0.5647 - footwear_output_acc: 0.5724 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7163 - val_loss: 9.0995 - val_gender_output_loss: 0.6847 - val_image_quality_output_loss: 0.9848 - val_age_output_loss: 1.4484 - val_weight_output_loss: 0.9859 - val_bag_output_loss: 0.9277 - val_footwear_output_loss: 1.2376 - val_pose_output_loss: 0.9298 - val_emotion_output_loss: 0.9552 - val_gender_output_acc: 0.5432 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5505 - val_footwear_output_acc: 0.3635 - val_pose_output_acc: 0.6156 - val_emotion_output_acc: 0.6880\n",
            "Epoch 7/100\n",
            "Learning rate:  0.9\n",
            "180/180 [==============================] - 142s 787ms/step - loss: 8.5119 - gender_output_loss: 0.6439 - image_quality_output_loss: 0.9764 - age_output_loss: 1.4042 - weight_output_loss: 0.9737 - bag_output_loss: 0.8944 - footwear_output_loss: 0.8933 - pose_output_loss: 0.9149 - emotion_output_loss: 0.8942 - gender_output_acc: 0.6156 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4043 - weight_output_acc: 0.6349 - bag_output_acc: 0.5657 - footwear_output_acc: 0.5951 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7162 - val_loss: 8.7126 - val_gender_output_loss: 0.6851 - val_image_quality_output_loss: 0.9743 - val_age_output_loss: 1.4593 - val_weight_output_loss: 0.9824 - val_bag_output_loss: 0.9383 - val_footwear_output_loss: 0.9082 - val_pose_output_loss: 0.9214 - val_emotion_output_loss: 0.9549 - val_gender_output_acc: 0.5521 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5198 - val_footwear_output_acc: 0.5958 - val_pose_output_acc: 0.6156 - val_emotion_output_acc: 0.6880\n",
            "Epoch 8/100\n",
            "Learning rate:  0.9\n",
            "180/180 [==============================] - 142s 788ms/step - loss: 8.4317 - gender_output_loss: 0.6334 - image_quality_output_loss: 0.9759 - age_output_loss: 1.4041 - weight_output_loss: 0.9755 - bag_output_loss: 0.8887 - footwear_output_loss: 0.8859 - pose_output_loss: 0.9138 - emotion_output_loss: 0.8923 - gender_output_acc: 0.6345 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4040 - weight_output_acc: 0.6346 - bag_output_acc: 0.5713 - footwear_output_acc: 0.6026 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7161 - val_loss: 8.5392 - val_gender_output_loss: 0.6324 - val_image_quality_output_loss: 0.9762 - val_age_output_loss: 1.4322 - val_weight_output_loss: 0.9813 - val_bag_output_loss: 0.9199 - val_footwear_output_loss: 0.8985 - val_pose_output_loss: 0.9118 - val_emotion_output_loss: 0.9508 - val_gender_output_acc: 0.6458 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5578 - val_footwear_output_acc: 0.5906 - val_pose_output_acc: 0.6156 - val_emotion_output_acc: 0.6880\n",
            "Epoch 9/100\n",
            "Learning rate:  0.9\n",
            "180/180 [==============================] - 141s 785ms/step - loss: 8.3574 - gender_output_loss: 0.6234 - image_quality_output_loss: 0.9750 - age_output_loss: 1.4031 - weight_output_loss: 0.9745 - bag_output_loss: 0.8866 - footwear_output_loss: 0.8821 - pose_output_loss: 0.9102 - emotion_output_loss: 0.8911 - gender_output_acc: 0.6389 - image_quality_output_acc: 0.5527 - age_output_acc: 0.4042 - weight_output_acc: 0.6346 - bag_output_acc: 0.5753 - footwear_output_acc: 0.6053 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7161 - val_loss: 8.5237 - val_gender_output_loss: 0.6684 - val_image_quality_output_loss: 0.9746 - val_age_output_loss: 1.4582 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.9263 - val_footwear_output_loss: 0.8573 - val_pose_output_loss: 0.9055 - val_emotion_output_loss: 0.9629 - val_gender_output_acc: 0.5755 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5677 - val_footwear_output_acc: 0.6328 - val_pose_output_acc: 0.6156 - val_emotion_output_acc: 0.6880\n",
            "Epoch 10/100\n",
            "Learning rate:  0.7\n",
            "180/180 [==============================] - 142s 788ms/step - loss: 8.2595 - gender_output_loss: 0.5951 - image_quality_output_loss: 0.9749 - age_output_loss: 1.4022 - weight_output_loss: 0.9761 - bag_output_loss: 0.8764 - footwear_output_loss: 0.8689 - pose_output_loss: 0.9052 - emotion_output_loss: 0.8914 - gender_output_acc: 0.6749 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4039 - weight_output_acc: 0.6345 - bag_output_acc: 0.5824 - footwear_output_acc: 0.6109 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7162 - val_loss: 8.2946 - val_gender_output_loss: 0.5775 - val_image_quality_output_loss: 0.9726 - val_age_output_loss: 1.4247 - val_weight_output_loss: 0.9817 - val_bag_output_loss: 0.8946 - val_footwear_output_loss: 0.8535 - val_pose_output_loss: 0.8912 - val_emotion_output_loss: 0.9471 - val_gender_output_acc: 0.6818 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5724 - val_footwear_output_acc: 0.6167 - val_pose_output_acc: 0.6156 - val_emotion_output_acc: 0.6880\n",
            "Epoch 11/100\n",
            "Learning rate:  0.7\n",
            "180/180 [==============================] - 138s 766ms/step - loss: 8.1942 - gender_output_loss: 0.5706 - image_quality_output_loss: 0.9733 - age_output_loss: 1.4012 - weight_output_loss: 0.9748 - bag_output_loss: 0.8758 - footwear_output_loss: 0.8671 - pose_output_loss: 0.9058 - emotion_output_loss: 0.8905 - gender_output_acc: 0.6974 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4040 - weight_output_acc: 0.6346 - bag_output_acc: 0.5845 - footwear_output_acc: 0.6095 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7161 - val_loss: 8.7865 - val_gender_output_loss: 0.8504 - val_image_quality_output_loss: 0.9760 - val_age_output_loss: 1.4869 - val_weight_output_loss: 0.9990 - val_bag_output_loss: 0.9437 - val_footwear_output_loss: 0.9220 - val_pose_output_loss: 0.9209 - val_emotion_output_loss: 0.9690 - val_gender_output_acc: 0.4536 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.4844 - val_footwear_output_acc: 0.5813 - val_pose_output_acc: 0.6156 - val_emotion_output_acc: 0.6880\n",
            "Epoch 12/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 142s 789ms/step - loss: 8.0524 - gender_output_loss: 0.5186 - image_quality_output_loss: 0.9702 - age_output_loss: 1.3951 - weight_output_loss: 0.9721 - bag_output_loss: 0.8640 - footwear_output_loss: 0.8325 - pose_output_loss: 0.8979 - emotion_output_loss: 0.8870 - gender_output_acc: 0.7379 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4044 - weight_output_acc: 0.6350 - bag_output_acc: 0.6005 - footwear_output_acc: 0.6280 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7163 - val_loss: 8.2935 - val_gender_output_loss: 0.5085 - val_image_quality_output_loss: 0.9687 - val_age_output_loss: 1.4222 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.8924 - val_footwear_output_loss: 0.9553 - val_pose_output_loss: 0.9080 - val_emotion_output_loss: 0.9432 - val_gender_output_acc: 0.7427 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5745 - val_footwear_output_acc: 0.5437 - val_pose_output_acc: 0.6156 - val_emotion_output_acc: 0.6880\n",
            "Epoch 13/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 143s 794ms/step - loss: 8.0072 - gender_output_loss: 0.4928 - image_quality_output_loss: 0.9701 - age_output_loss: 1.3934 - weight_output_loss: 0.9746 - bag_output_loss: 0.8605 - footwear_output_loss: 0.8268 - pose_output_loss: 0.8946 - emotion_output_loss: 0.8863 - gender_output_acc: 0.7581 - image_quality_output_acc: 0.5527 - age_output_acc: 0.4043 - weight_output_acc: 0.6346 - bag_output_acc: 0.6045 - footwear_output_acc: 0.6333 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7161 - val_loss: 8.2372 - val_gender_output_loss: 0.4840 - val_image_quality_output_loss: 0.9702 - val_age_output_loss: 1.4180 - val_weight_output_loss: 0.9824 - val_bag_output_loss: 0.8933 - val_footwear_output_loss: 0.9438 - val_pose_output_loss: 0.8981 - val_emotion_output_loss: 0.9428 - val_gender_output_acc: 0.7625 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5750 - val_footwear_output_acc: 0.5406 - val_pose_output_acc: 0.6156 - val_emotion_output_acc: 0.6880\n",
            "Epoch 14/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 142s 791ms/step - loss: 7.9748 - gender_output_loss: 0.4742 - image_quality_output_loss: 0.9710 - age_output_loss: 1.3922 - weight_output_loss: 0.9741 - bag_output_loss: 0.8595 - footwear_output_loss: 0.8264 - pose_output_loss: 0.8898 - emotion_output_loss: 0.8864 - gender_output_acc: 0.7667 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4040 - weight_output_acc: 0.6347 - bag_output_acc: 0.6063 - footwear_output_acc: 0.6301 - pose_output_acc: 0.6189 - emotion_output_acc: 0.7160 - val_loss: 8.0374 - val_gender_output_loss: 0.4642 - val_image_quality_output_loss: 0.9690 - val_age_output_loss: 1.4167 - val_weight_output_loss: 0.9812 - val_bag_output_loss: 0.8822 - val_footwear_output_loss: 0.8059 - val_pose_output_loss: 0.8787 - val_emotion_output_loss: 0.9416 - val_gender_output_acc: 0.7776 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5938 - val_footwear_output_acc: 0.6375 - val_pose_output_acc: 0.6156 - val_emotion_output_acc: 0.6880\n",
            "Epoch 15/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 784ms/step - loss: 7.9342 - gender_output_loss: 0.4624 - image_quality_output_loss: 0.9683 - age_output_loss: 1.3903 - weight_output_loss: 0.9722 - bag_output_loss: 0.8570 - footwear_output_loss: 0.8228 - pose_output_loss: 0.8823 - emotion_output_loss: 0.8844 - gender_output_acc: 0.7783 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4043 - weight_output_acc: 0.6348 - bag_output_acc: 0.6079 - footwear_output_acc: 0.6308 - pose_output_acc: 0.6205 - emotion_output_acc: 0.7162 - val_loss: 8.3003 - val_gender_output_loss: 0.6364 - val_image_quality_output_loss: 0.9720 - val_age_output_loss: 1.4404 - val_weight_output_loss: 0.9855 - val_bag_output_loss: 0.9122 - val_footwear_output_loss: 0.8251 - val_pose_output_loss: 0.8892 - val_emotion_output_loss: 0.9480 - val_gender_output_acc: 0.6792 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5500 - val_footwear_output_acc: 0.6297 - val_pose_output_acc: 0.6156 - val_emotion_output_acc: 0.6880\n",
            "Epoch 16/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 138s 768ms/step - loss: 7.9077 - gender_output_loss: 0.4531 - image_quality_output_loss: 0.9695 - age_output_loss: 1.3920 - weight_output_loss: 0.9738 - bag_output_loss: 0.8550 - footwear_output_loss: 0.8184 - pose_output_loss: 0.8739 - emotion_output_loss: 0.8838 - gender_output_acc: 0.7824 - image_quality_output_acc: 0.5527 - age_output_acc: 0.4045 - weight_output_acc: 0.6346 - bag_output_acc: 0.6135 - footwear_output_acc: 0.6382 - pose_output_acc: 0.6216 - emotion_output_acc: 0.7163 - val_loss: 8.0150 - val_gender_output_loss: 0.4535 - val_image_quality_output_loss: 0.9699 - val_age_output_loss: 1.4215 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.8866 - val_footwear_output_loss: 0.7972 - val_pose_output_loss: 0.8762 - val_emotion_output_loss: 0.9416 - val_gender_output_acc: 0.7818 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5891 - val_footwear_output_acc: 0.6469 - val_pose_output_acc: 0.6161 - val_emotion_output_acc: 0.6880\n",
            "Epoch 17/100\n",
            "Learning rate:  0.06999999999999999\n",
            "180/180 [==============================] - 140s 776ms/step - loss: 7.8485 - gender_output_loss: 0.4293 - image_quality_output_loss: 0.9669 - age_output_loss: 1.3909 - weight_output_loss: 0.9732 - bag_output_loss: 0.8549 - footwear_output_loss: 0.8035 - pose_output_loss: 0.8619 - emotion_output_loss: 0.8843 - gender_output_acc: 0.7975 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4044 - weight_output_acc: 0.6350 - bag_output_acc: 0.6122 - footwear_output_acc: 0.6458 - pose_output_acc: 0.6240 - emotion_output_acc: 0.7161 - val_loss: 7.9514 - val_gender_output_loss: 0.4321 - val_image_quality_output_loss: 0.9656 - val_age_output_loss: 1.4214 - val_weight_output_loss: 0.9827 - val_bag_output_loss: 0.8794 - val_footwear_output_loss: 0.7998 - val_pose_output_loss: 0.8463 - val_emotion_output_loss: 0.9423 - val_gender_output_acc: 0.7943 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5958 - val_footwear_output_acc: 0.6427 - val_pose_output_acc: 0.6203 - val_emotion_output_acc: 0.6880\n",
            "Epoch 18/100\n",
            "Learning rate:  0.06999999999999999\n",
            "180/180 [==============================] - 141s 782ms/step - loss: 7.8243 - gender_output_loss: 0.4247 - image_quality_output_loss: 0.9662 - age_output_loss: 1.3917 - weight_output_loss: 0.9741 - bag_output_loss: 0.8518 - footwear_output_loss: 0.8000 - pose_output_loss: 0.8523 - emotion_output_loss: 0.8831 - gender_output_acc: 0.8014 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4044 - weight_output_acc: 0.6347 - bag_output_acc: 0.6168 - footwear_output_acc: 0.6463 - pose_output_acc: 0.6301 - emotion_output_acc: 0.7161 - val_loss: 7.9579 - val_gender_output_loss: 0.4209 - val_image_quality_output_loss: 0.9689 - val_age_output_loss: 1.4233 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.8791 - val_footwear_output_loss: 0.7974 - val_pose_output_loss: 0.8642 - val_emotion_output_loss: 0.9413 - val_gender_output_acc: 0.7964 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6010 - val_footwear_output_acc: 0.6479 - val_pose_output_acc: 0.6177 - val_emotion_output_acc: 0.6880\n",
            "Epoch 19/100\n",
            "Learning rate:  0.06999999999999999\n",
            "180/180 [==============================] - 141s 783ms/step - loss: 7.7944 - gender_output_loss: 0.4112 - image_quality_output_loss: 0.9667 - age_output_loss: 1.3884 - weight_output_loss: 0.9738 - bag_output_loss: 0.8518 - footwear_output_loss: 0.8017 - pose_output_loss: 0.8410 - emotion_output_loss: 0.8829 - gender_output_acc: 0.8075 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4043 - weight_output_acc: 0.6348 - bag_output_acc: 0.6148 - footwear_output_acc: 0.6434 - pose_output_acc: 0.6358 - emotion_output_acc: 0.7161 - val_loss: 7.9614 - val_gender_output_loss: 0.4550 - val_image_quality_output_loss: 0.9698 - val_age_output_loss: 1.4234 - val_weight_output_loss: 0.9838 - val_bag_output_loss: 0.8778 - val_footwear_output_loss: 0.8004 - val_pose_output_loss: 0.8343 - val_emotion_output_loss: 0.9413 - val_gender_output_acc: 0.7802 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5974 - val_footwear_output_acc: 0.6469 - val_pose_output_acc: 0.6281 - val_emotion_output_acc: 0.6880\n",
            "Epoch 20/100\n",
            "Learning rate:  0.06999999999999999\n",
            "180/180 [==============================] - 141s 783ms/step - loss: 7.7741 - gender_output_loss: 0.4072 - image_quality_output_loss: 0.9681 - age_output_loss: 1.3905 - weight_output_loss: 0.9741 - bag_output_loss: 0.8486 - footwear_output_loss: 0.8030 - pose_output_loss: 0.8271 - emotion_output_loss: 0.8815 - gender_output_acc: 0.8132 - image_quality_output_acc: 0.5526 - age_output_acc: 0.4041 - weight_output_acc: 0.6350 - bag_output_acc: 0.6186 - footwear_output_acc: 0.6467 - pose_output_acc: 0.6402 - emotion_output_acc: 0.7161 - val_loss: 7.9176 - val_gender_output_loss: 0.4265 - val_image_quality_output_loss: 0.9684 - val_age_output_loss: 1.4236 - val_weight_output_loss: 0.9840 - val_bag_output_loss: 0.8764 - val_footwear_output_loss: 0.7951 - val_pose_output_loss: 0.8309 - val_emotion_output_loss: 0.9403 - val_gender_output_acc: 0.8109 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6010 - val_footwear_output_acc: 0.6443 - val_pose_output_acc: 0.6292 - val_emotion_output_acc: 0.6880\n",
            "Epoch 21/100\n",
            "Learning rate:  0.06999999999999999\n",
            "180/180 [==============================] - 136s 756ms/step - loss: 7.7401 - gender_output_loss: 0.4018 - image_quality_output_loss: 0.9677 - age_output_loss: 1.3903 - weight_output_loss: 0.9745 - bag_output_loss: 0.8473 - footwear_output_loss: 0.8012 - pose_output_loss: 0.8047 - emotion_output_loss: 0.8817 - gender_output_acc: 0.8149 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6347 - bag_output_acc: 0.6167 - footwear_output_acc: 0.6433 - pose_output_acc: 0.6416 - emotion_output_acc: 0.7161 - val_loss: 7.8789 - val_gender_output_loss: 0.4066 - val_image_quality_output_loss: 0.9695 - val_age_output_loss: 1.4194 - val_weight_output_loss: 0.9840 - val_bag_output_loss: 0.8771 - val_footwear_output_loss: 0.8046 - val_pose_output_loss: 0.8049 - val_emotion_output_loss: 0.9433 - val_gender_output_acc: 0.8203 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5958 - val_footwear_output_acc: 0.6531 - val_pose_output_acc: 0.6505 - val_emotion_output_acc: 0.6880\n",
            "Epoch 22/100\n",
            "Learning rate:  0.06999999999999999\n",
            "180/180 [==============================] - 142s 789ms/step - loss: 7.6965 - gender_output_loss: 0.3915 - image_quality_output_loss: 0.9686 - age_output_loss: 1.3895 - weight_output_loss: 0.9751 - bag_output_loss: 0.8507 - footwear_output_loss: 0.7988 - pose_output_loss: 0.7756 - emotion_output_loss: 0.8788 - gender_output_acc: 0.8244 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4042 - weight_output_acc: 0.6345 - bag_output_acc: 0.6181 - footwear_output_acc: 0.6479 - pose_output_acc: 0.6546 - emotion_output_acc: 0.7163 - val_loss: 8.0627 - val_gender_output_loss: 0.4468 - val_image_quality_output_loss: 0.9710 - val_age_output_loss: 1.4363 - val_weight_output_loss: 0.9852 - val_bag_output_loss: 0.8920 - val_footwear_output_loss: 0.8304 - val_pose_output_loss: 0.8905 - val_emotion_output_loss: 0.9441 - val_gender_output_acc: 0.7911 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5807 - val_footwear_output_acc: 0.6385 - val_pose_output_acc: 0.6240 - val_emotion_output_acc: 0.6880\n",
            "Epoch 23/100\n",
            "Learning rate:  0.06999999999999999\n",
            "180/180 [==============================] - 141s 786ms/step - loss: 7.6703 - gender_output_loss: 0.3935 - image_quality_output_loss: 0.9688 - age_output_loss: 1.3902 - weight_output_loss: 0.9744 - bag_output_loss: 0.8470 - footwear_output_loss: 0.7987 - pose_output_loss: 0.7535 - emotion_output_loss: 0.8793 - gender_output_acc: 0.8208 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4042 - weight_output_acc: 0.6349 - bag_output_acc: 0.6182 - footwear_output_acc: 0.6430 - pose_output_acc: 0.6603 - emotion_output_acc: 0.7161 - val_loss: 7.9752 - val_gender_output_loss: 0.4606 - val_image_quality_output_loss: 0.9699 - val_age_output_loss: 1.4337 - val_weight_output_loss: 0.9883 - val_bag_output_loss: 0.8867 - val_footwear_output_loss: 0.8127 - val_pose_output_loss: 0.8171 - val_emotion_output_loss: 0.9427 - val_gender_output_acc: 0.7818 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5927 - val_footwear_output_acc: 0.6411 - val_pose_output_acc: 0.6333 - val_emotion_output_acc: 0.6880\n",
            "Epoch 24/100\n",
            "Learning rate:  0.06999999999999999\n",
            "180/180 [==============================] - 141s 784ms/step - loss: 7.6251 - gender_output_loss: 0.3784 - image_quality_output_loss: 0.9682 - age_output_loss: 1.3919 - weight_output_loss: 0.9754 - bag_output_loss: 0.8464 - footwear_output_loss: 0.7999 - pose_output_loss: 0.7250 - emotion_output_loss: 0.8778 - gender_output_acc: 0.8304 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.6218 - footwear_output_acc: 0.6437 - pose_output_acc: 0.6710 - emotion_output_acc: 0.7163 - val_loss: 7.8764 - val_gender_output_loss: 0.4595 - val_image_quality_output_loss: 0.9699 - val_age_output_loss: 1.4317 - val_weight_output_loss: 0.9876 - val_bag_output_loss: 0.8827 - val_footwear_output_loss: 0.8061 - val_pose_output_loss: 0.7411 - val_emotion_output_loss: 0.9372 - val_gender_output_acc: 0.7901 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5875 - val_footwear_output_acc: 0.6406 - val_pose_output_acc: 0.6630 - val_emotion_output_acc: 0.6880\n",
            "Epoch 25/100\n",
            "Learning rate:  0.06999999999999999\n",
            "180/180 [==============================] - 140s 781ms/step - loss: 7.6076 - gender_output_loss: 0.3827 - image_quality_output_loss: 0.9696 - age_output_loss: 1.3898 - weight_output_loss: 0.9742 - bag_output_loss: 0.8435 - footwear_output_loss: 0.7998 - pose_output_loss: 0.7120 - emotion_output_loss: 0.8768 - gender_output_acc: 0.8335 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4044 - weight_output_acc: 0.6346 - bag_output_acc: 0.6243 - footwear_output_acc: 0.6474 - pose_output_acc: 0.6712 - emotion_output_acc: 0.7162 - val_loss: 7.8983 - val_gender_output_loss: 0.5133 - val_image_quality_output_loss: 0.9689 - val_age_output_loss: 1.4337 - val_weight_output_loss: 0.9849 - val_bag_output_loss: 0.9052 - val_footwear_output_loss: 0.8079 - val_pose_output_loss: 0.6880 - val_emotion_output_loss: 0.9384 - val_gender_output_acc: 0.7531 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5849 - val_footwear_output_acc: 0.6422 - val_pose_output_acc: 0.6786 - val_emotion_output_acc: 0.6880\n",
            "Epoch 26/100\n",
            "Learning rate:  0.06999999999999999\n",
            "180/180 [==============================] - 138s 765ms/step - loss: 7.5515 - gender_output_loss: 0.3642 - image_quality_output_loss: 0.9687 - age_output_loss: 1.3886 - weight_output_loss: 0.9732 - bag_output_loss: 0.8433 - footwear_output_loss: 0.7956 - pose_output_loss: 0.6860 - emotion_output_loss: 0.8754 - gender_output_acc: 0.8396 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4039 - weight_output_acc: 0.6345 - bag_output_acc: 0.6246 - footwear_output_acc: 0.6492 - pose_output_acc: 0.6872 - emotion_output_acc: 0.7162 - val_loss: 7.8099 - val_gender_output_loss: 0.5060 - val_image_quality_output_loss: 0.9681 - val_age_output_loss: 1.4236 - val_weight_output_loss: 0.9836 - val_bag_output_loss: 0.8768 - val_footwear_output_loss: 0.7925 - val_pose_output_loss: 0.6677 - val_emotion_output_loss: 0.9362 - val_gender_output_acc: 0.7677 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5885 - val_footwear_output_acc: 0.6505 - val_pose_output_acc: 0.6937 - val_emotion_output_acc: 0.6880\n",
            "Epoch 27/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 782ms/step - loss: 7.6255 - gender_output_loss: 0.4003 - image_quality_output_loss: 0.9697 - age_output_loss: 1.3904 - weight_output_loss: 0.9749 - bag_output_loss: 0.8523 - footwear_output_loss: 0.8080 - pose_output_loss: 0.6996 - emotion_output_loss: 0.8772 - gender_output_acc: 0.8132 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4041 - weight_output_acc: 0.6344 - bag_output_acc: 0.6120 - footwear_output_acc: 0.6432 - pose_output_acc: 0.6875 - emotion_output_acc: 0.7161 - val_loss: 8.1100 - val_gender_output_loss: 0.4548 - val_image_quality_output_loss: 0.9701 - val_age_output_loss: 1.4264 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.8798 - val_footwear_output_loss: 0.8345 - val_pose_output_loss: 0.9580 - val_emotion_output_loss: 0.9515 - val_gender_output_acc: 0.7833 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6073 - val_footwear_output_acc: 0.6417 - val_pose_output_acc: 0.5510 - val_emotion_output_acc: 0.6880\n",
            "Epoch 28/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 784ms/step - loss: 7.5782 - gender_output_loss: 0.3866 - image_quality_output_loss: 0.9718 - age_output_loss: 1.3917 - weight_output_loss: 0.9753 - bag_output_loss: 0.8460 - footwear_output_loss: 0.8029 - pose_output_loss: 0.6816 - emotion_output_loss: 0.8738 - gender_output_acc: 0.8241 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4041 - weight_output_acc: 0.6349 - bag_output_acc: 0.6214 - footwear_output_acc: 0.6485 - pose_output_acc: 0.6983 - emotion_output_acc: 0.7159 - val_loss: 7.8859 - val_gender_output_loss: 0.4635 - val_image_quality_output_loss: 0.9732 - val_age_output_loss: 1.4274 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.8832 - val_footwear_output_loss: 0.7870 - val_pose_output_loss: 0.7857 - val_emotion_output_loss: 0.9362 - val_gender_output_acc: 0.7828 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5948 - val_footwear_output_acc: 0.6620 - val_pose_output_acc: 0.6370 - val_emotion_output_acc: 0.6880\n",
            "Epoch 29/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 781ms/step - loss: 7.5389 - gender_output_loss: 0.3795 - image_quality_output_loss: 0.9713 - age_output_loss: 1.3904 - weight_output_loss: 0.9745 - bag_output_loss: 0.8433 - footwear_output_loss: 0.7997 - pose_output_loss: 0.6645 - emotion_output_loss: 0.8720 - gender_output_acc: 0.8318 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4041 - weight_output_acc: 0.6347 - bag_output_acc: 0.6269 - footwear_output_acc: 0.6445 - pose_output_acc: 0.7064 - emotion_output_acc: 0.7159 - val_loss: 7.8849 - val_gender_output_loss: 0.4763 - val_image_quality_output_loss: 0.9720 - val_age_output_loss: 1.4266 - val_weight_output_loss: 0.9877 - val_bag_output_loss: 0.8894 - val_footwear_output_loss: 0.8744 - val_pose_output_loss: 0.6884 - val_emotion_output_loss: 0.9286 - val_gender_output_acc: 0.7792 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5849 - val_footwear_output_acc: 0.6057 - val_pose_output_acc: 0.6969 - val_emotion_output_acc: 0.6880\n",
            "Epoch 30/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 140s 778ms/step - loss: 7.4842 - gender_output_loss: 0.3681 - image_quality_output_loss: 0.9718 - age_output_loss: 1.3880 - weight_output_loss: 0.9739 - bag_output_loss: 0.8410 - footwear_output_loss: 0.7940 - pose_output_loss: 0.6387 - emotion_output_loss: 0.8695 - gender_output_acc: 0.8369 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4068 - weight_output_acc: 0.6346 - bag_output_acc: 0.6269 - footwear_output_acc: 0.6518 - pose_output_acc: 0.7231 - emotion_output_acc: 0.7159 - val_loss: 7.6847 - val_gender_output_loss: 0.4374 - val_image_quality_output_loss: 0.9702 - val_age_output_loss: 1.4252 - val_weight_output_loss: 0.9859 - val_bag_output_loss: 0.8717 - val_footwear_output_loss: 0.7793 - val_pose_output_loss: 0.6526 - val_emotion_output_loss: 0.9257 - val_gender_output_acc: 0.7995 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6010 - val_footwear_output_acc: 0.6557 - val_pose_output_acc: 0.7214 - val_emotion_output_acc: 0.6880\n",
            "Epoch 31/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 138s 768ms/step - loss: 7.4461 - gender_output_loss: 0.3511 - image_quality_output_loss: 0.9713 - age_output_loss: 1.3904 - weight_output_loss: 0.9742 - bag_output_loss: 0.8405 - footwear_output_loss: 0.7898 - pose_output_loss: 0.6271 - emotion_output_loss: 0.8669 - gender_output_acc: 0.8501 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4056 - weight_output_acc: 0.6347 - bag_output_acc: 0.6207 - footwear_output_acc: 0.6510 - pose_output_acc: 0.7273 - emotion_output_acc: 0.7162 - val_loss: 7.6440 - val_gender_output_loss: 0.4010 - val_image_quality_output_loss: 0.9741 - val_age_output_loss: 1.4238 - val_weight_output_loss: 0.9870 - val_bag_output_loss: 0.8868 - val_footwear_output_loss: 0.7794 - val_pose_output_loss: 0.6338 - val_emotion_output_loss: 0.9258 - val_gender_output_acc: 0.8344 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3625 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5938 - val_footwear_output_acc: 0.6615 - val_pose_output_acc: 0.7370 - val_emotion_output_acc: 0.6880\n",
            "Epoch 32/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 138s 768ms/step - loss: 7.4461 - gender_output_loss: 0.3511 - image_quality_output_loss: 0.9713 - age_output_loss: 1.3904 - weight_output_loss: 0.9742 - bag_output_loss: 0.8405 - footwear_output_loss: 0.7898 - pose_output_loss: 0.6271 - emotion_output_loss: 0.8669 - gender_output_acc: 0.8501 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4056 - weight_output_acc: 0.6347 - bag_output_acc: 0.6207 - footwear_output_acc: 0.6510 - pose_output_acc: 0.7273 - emotion_output_acc: 0.7162 - val_loss: 7.6440 - val_gender_output_loss: 0.4010 - val_image_quality_output_loss: 0.9741 - val_age_output_loss: 1.4238 - val_weight_output_loss: 0.9870 - val_bag_output_loss: 0.8868 - val_footwear_output_loss: 0.7794 - val_pose_output_loss: 0.6338 - val_emotion_output_loss: 0.9258 - val_gender_output_acc: 0.8344 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3625 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5938 - val_footwear_output_acc: 0.6615 - val_pose_output_acc: 0.7370 - val_emotion_output_acc: 0.6880\n",
            "180/180 [==============================] - 141s 785ms/step - loss: 7.4160 - gender_output_loss: 0.3461 - image_quality_output_loss: 0.9708 - age_output_loss: 1.3879 - weight_output_loss: 0.9744 - bag_output_loss: 0.8387 - footwear_output_loss: 0.7884 - pose_output_loss: 0.6138 - emotion_output_loss: 0.8658 - gender_output_acc: 0.8498 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4056 - weight_output_acc: 0.6348 - bag_output_acc: 0.6242 - footwear_output_acc: 0.6506 - pose_output_acc: 0.7359 - emotion_output_acc: 0.7161 - val_loss: 7.6995 - val_gender_output_loss: 0.4486 - val_image_quality_output_loss: 0.9717 - val_age_output_loss: 1.4194 - val_weight_output_loss: 0.9846 - val_bag_output_loss: 0.8945 - val_footwear_output_loss: 0.8308 - val_pose_output_loss: 0.6047 - val_emotion_output_loss: 0.9171 - val_gender_output_acc: 0.7969 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3646 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5828 - val_footwear_output_acc: 0.6333 - val_pose_output_acc: 0.7391 - val_emotion_output_acc: 0.6880\n",
            "Epoch 33/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 140s 779ms/step - loss: 7.3791 - gender_output_loss: 0.3296 - image_quality_output_loss: 0.9711 - age_output_loss: 1.3871 - weight_output_loss: 0.9744 - bag_output_loss: 0.8379 - footwear_output_loss: 0.7836 - pose_output_loss: 0.6037 - emotion_output_loss: 0.8658 - gender_output_acc: 0.8583 - image_quality_output_acc: 0.5539 - age_output_acc: 0.4057 - weight_output_acc: 0.6342 - bag_output_acc: 0.6255 - footwear_output_acc: 0.6536 - pose_output_acc: 0.7347 - emotion_output_acc: 0.7161 - val_loss: 8.4917 - val_gender_output_loss: 0.5478 - val_image_quality_output_loss: 0.9713 - val_age_output_loss: 1.4222 - val_weight_output_loss: 0.9845 - val_bag_output_loss: 0.9390 - val_footwear_output_loss: 0.8071 - val_pose_output_loss: 1.2323 - val_emotion_output_loss: 0.9639 - val_gender_output_acc: 0.7635 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3630 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5750 - val_footwear_output_acc: 0.6531 - val_pose_output_acc: 0.4417 - val_emotion_output_acc: 0.6880\n",
            "Epoch 34/100\n",
            "Learning rate:  0.13999999999999999\n",
            "179/180 [============================>.] - ETA: 0s - loss: 7.3629 - gender_output_loss: 0.3270 - image_quality_output_loss: 0.9702 - age_output_loss: 1.3860 - weight_output_loss: 0.9739 - bag_output_loss: 0.8391 - footwear_output_loss: 0.7814 - pose_output_loss: 0.5994 - emotion_output_loss: 0.8643 - gender_output_acc: 0.8612 - image_quality_output_acc: 0.5534 - age_output_acc: 0.4078 - weight_output_acc: 0.6349 - bag_output_acc: 0.6260 - footwear_output_acc: 0.6579 - pose_output_acc: 0.7390 - emotion_output_acc: 0.7163Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 142s 788ms/step - loss: 7.3632 - gender_output_loss: 0.3271 - image_quality_output_loss: 0.9707 - age_output_loss: 1.3860 - weight_output_loss: 0.9739 - bag_output_loss: 0.8392 - footwear_output_loss: 0.7813 - pose_output_loss: 0.5990 - emotion_output_loss: 0.8644 - gender_output_acc: 0.8613 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4082 - weight_output_acc: 0.6347 - bag_output_acc: 0.6258 - footwear_output_acc: 0.6577 - pose_output_acc: 0.7391 - emotion_output_acc: 0.7161 - val_loss: 7.6044 - val_gender_output_loss: 0.3884 - val_image_quality_output_loss: 0.9714 - val_age_output_loss: 1.4186 - val_weight_output_loss: 0.9826 - val_bag_output_loss: 0.8917 - val_footwear_output_loss: 0.7975 - val_pose_output_loss: 0.6131 - val_emotion_output_loss: 0.9217 - val_gender_output_acc: 0.8339 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3635 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5974 - val_footwear_output_acc: 0.6438 - val_pose_output_acc: 0.7302 - val_emotion_output_acc: 0.6880\n",
            "Epoch 35/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 140s 779ms/step - loss: 7.3285 - gender_output_loss: 0.3169 - image_quality_output_loss: 0.9702 - age_output_loss: 1.3854 - weight_output_loss: 0.9731 - bag_output_loss: 0.8363 - footwear_output_loss: 0.7787 - pose_output_loss: 0.5869 - emotion_output_loss: 0.8636 - gender_output_acc: 0.8692 - image_quality_output_acc: 0.5546 - age_output_acc: 0.4067 - weight_output_acc: 0.6346 - bag_output_acc: 0.6208 - footwear_output_acc: 0.6560 - pose_output_acc: 0.7501 - emotion_output_acc: 0.7161 - val_loss: 7.7603 - val_gender_output_loss: 0.4751 - val_image_quality_output_loss: 0.9688 - val_age_output_loss: 1.4184 - val_weight_output_loss: 0.9861 - val_bag_output_loss: 0.8812 - val_footwear_output_loss: 0.8620 - val_pose_output_loss: 0.6333 - val_emotion_output_loss: 0.9200 - val_gender_output_acc: 0.7792 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3635 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5943 - val_footwear_output_acc: 0.6073 - val_pose_output_acc: 0.7391 - val_emotion_output_acc: 0.6880\n",
            "Epoch 36/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 139s 769ms/step - loss: 7.3000 - gender_output_loss: 0.3058 - image_quality_output_loss: 0.9720 - age_output_loss: 1.3885 - weight_output_loss: 0.9734 - bag_output_loss: 0.8318 - footwear_output_loss: 0.7829 - pose_output_loss: 0.5688 - emotion_output_loss: 0.8635 - gender_output_acc: 0.8673 - image_quality_output_acc: 0.5538 - age_output_acc: 0.4069 - weight_output_acc: 0.6351 - bag_output_acc: 0.6280 - footwear_output_acc: 0.6517 - pose_output_acc: 0.7576 - emotion_output_acc: 0.7158 - val_loss: 7.7127 - val_gender_output_loss: 0.4733 - val_image_quality_output_loss: 0.9713 - val_age_output_loss: 1.4186 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.8801 - val_footwear_output_loss: 0.8123 - val_pose_output_loss: 0.6457 - val_emotion_output_loss: 0.9171 - val_gender_output_acc: 0.8094 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6026 - val_footwear_output_acc: 0.6391 - val_pose_output_acc: 0.7302 - val_emotion_output_acc: 0.6880\n",
            "Epoch 37/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 783ms/step - loss: 7.2708 - gender_output_loss: 0.2999 - image_quality_output_loss: 0.9713 - age_output_loss: 1.3841 - weight_output_loss: 0.9735 - bag_output_loss: 0.8358 - footwear_output_loss: 0.7711 - pose_output_loss: 0.5672 - emotion_output_loss: 0.8585 - gender_output_acc: 0.8774 - image_quality_output_acc: 0.5536 - age_output_acc: 0.4062 - weight_output_acc: 0.6345 - bag_output_acc: 0.6286 - footwear_output_acc: 0.6648 - pose_output_acc: 0.7570 - emotion_output_acc: 0.7161 - val_loss: 7.5732 - val_gender_output_loss: 0.3780 - val_image_quality_output_loss: 0.9727 - val_age_output_loss: 1.4124 - val_weight_output_loss: 0.9860 - val_bag_output_loss: 0.8763 - val_footwear_output_loss: 0.8280 - val_pose_output_loss: 0.6011 - val_emotion_output_loss: 0.9114 - val_gender_output_acc: 0.8391 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3651 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5953 - val_footwear_output_acc: 0.6297 - val_pose_output_acc: 0.7484 - val_emotion_output_acc: 0.6880\n",
            "Epoch 38/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 785ms/step - loss: 7.2503 - gender_output_loss: 0.3031 - image_quality_output_loss: 0.9715 - age_output_loss: 1.3847 - weight_output_loss: 0.9736 - bag_output_loss: 0.8327 - footwear_output_loss: 0.7608 - pose_output_loss: 0.5594 - emotion_output_loss: 0.8588 - gender_output_acc: 0.8753 - image_quality_output_acc: 0.5534 - age_output_acc: 0.4089 - weight_output_acc: 0.6345 - bag_output_acc: 0.6286 - footwear_output_acc: 0.6694 - pose_output_acc: 0.7642 - emotion_output_acc: 0.7162 - val_loss: 7.7213 - val_gender_output_loss: 0.4420 - val_image_quality_output_loss: 0.9706 - val_age_output_loss: 1.4191 - val_weight_output_loss: 0.9815 - val_bag_output_loss: 0.8685 - val_footwear_output_loss: 0.8088 - val_pose_output_loss: 0.7059 - val_emotion_output_loss: 0.9211 - val_gender_output_acc: 0.8109 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6057 - val_footwear_output_acc: 0.6401 - val_pose_output_acc: 0.6734 - val_emotion_output_acc: 0.6880\n",
            "Epoch 39/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 142s 788ms/step - loss: 7.2106 - gender_output_loss: 0.2756 - image_quality_output_loss: 0.9716 - age_output_loss: 1.3856 - weight_output_loss: 0.9740 - bag_output_loss: 0.8345 - footwear_output_loss: 0.7674 - pose_output_loss: 0.5433 - emotion_output_loss: 0.8567 - gender_output_acc: 0.8888 - image_quality_output_acc: 0.5540 - age_output_acc: 0.4083 - weight_output_acc: 0.6345 - bag_output_acc: 0.6282 - footwear_output_acc: 0.6642 - pose_output_acc: 0.7682 - emotion_output_acc: 0.7163 - val_loss: 7.7970 - val_gender_output_loss: 0.3902 - val_image_quality_output_loss: 0.9711 - val_age_output_loss: 1.4208 - val_weight_output_loss: 0.9831 - val_bag_output_loss: 0.8773 - val_footwear_output_loss: 0.7950 - val_pose_output_loss: 0.8200 - val_emotion_output_loss: 0.9396 - val_gender_output_acc: 0.8323 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3625 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5911 - val_footwear_output_acc: 0.6474 - val_pose_output_acc: 0.6432 - val_emotion_output_acc: 0.6880\n",
            "Epoch 40/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 785ms/step - loss: 7.1857 - gender_output_loss: 0.2788 - image_quality_output_loss: 0.9709 - age_output_loss: 1.3828 - weight_output_loss: 0.9735 - bag_output_loss: 0.8311 - footwear_output_loss: 0.7638 - pose_output_loss: 0.5314 - emotion_output_loss: 0.8550 - gender_output_acc: 0.8889 - image_quality_output_acc: 0.5539 - age_output_acc: 0.4069 - weight_output_acc: 0.6346 - bag_output_acc: 0.6299 - footwear_output_acc: 0.6661 - pose_output_acc: 0.7806 - emotion_output_acc: 0.7166 - val_loss: 7.6965 - val_gender_output_loss: 0.4367 - val_image_quality_output_loss: 0.9732 - val_age_output_loss: 1.4184 - val_weight_output_loss: 0.9874 - val_bag_output_loss: 0.8898 - val_footwear_output_loss: 0.8026 - val_pose_output_loss: 0.6753 - val_emotion_output_loss: 0.9166 - val_gender_output_acc: 0.8245 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3682 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6005 - val_footwear_output_acc: 0.6474 - val_pose_output_acc: 0.7312 - val_emotion_output_acc: 0.6880\n",
            "Epoch 41/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 138s 767ms/step - loss: 7.1620 - gender_output_loss: 0.2755 - image_quality_output_loss: 0.9703 - age_output_loss: 1.3835 - weight_output_loss: 0.9724 - bag_output_loss: 0.8284 - footwear_output_loss: 0.7631 - pose_output_loss: 0.5162 - emotion_output_loss: 0.8577 - gender_output_acc: 0.8884 - image_quality_output_acc: 0.5543 - age_output_acc: 0.4089 - weight_output_acc: 0.6347 - bag_output_acc: 0.6273 - footwear_output_acc: 0.6623 - pose_output_acc: 0.7885 - emotion_output_acc: 0.7162 - val_loss: 7.6560 - val_gender_output_loss: 0.3712 - val_image_quality_output_loss: 0.9747 - val_age_output_loss: 1.4223 - val_weight_output_loss: 0.9833 - val_bag_output_loss: 0.8787 - val_footwear_output_loss: 0.8234 - val_pose_output_loss: 0.6889 - val_emotion_output_loss: 0.9203 - val_gender_output_acc: 0.8380 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3682 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6016 - val_footwear_output_acc: 0.6245 - val_pose_output_acc: 0.7031 - val_emotion_output_acc: 0.6880\n",
            "Epoch 42/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 785ms/step - loss: 7.1351 - gender_output_loss: 0.2639 - image_quality_output_loss: 0.9712 - age_output_loss: 1.3839 - weight_output_loss: 0.9718 - bag_output_loss: 0.8307 - footwear_output_loss: 0.7558 - pose_output_loss: 0.5109 - emotion_output_loss: 0.8553 - gender_output_acc: 0.8957 - image_quality_output_acc: 0.5541 - age_output_acc: 0.4077 - weight_output_acc: 0.6343 - bag_output_acc: 0.6322 - footwear_output_acc: 0.6690 - pose_output_acc: 0.7927 - emotion_output_acc: 0.7161 - val_loss: 7.3987 - val_gender_output_loss: 0.3611 - val_image_quality_output_loss: 0.9708 - val_age_output_loss: 1.4125 - val_weight_output_loss: 0.9823 - val_bag_output_loss: 0.8737 - val_footwear_output_loss: 0.7807 - val_pose_output_loss: 0.5174 - val_emotion_output_loss: 0.9104 - val_gender_output_acc: 0.8594 - val_image_quality_output_acc: 0.5547 - val_age_output_acc: 0.3651 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5911 - val_footwear_output_acc: 0.6547 - val_pose_output_acc: 0.8000 - val_emotion_output_acc: 0.6880\n",
            "Epoch 43/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 140s 780ms/step - loss: 7.1224 - gender_output_loss: 0.2596 - image_quality_output_loss: 0.9704 - age_output_loss: 1.3842 - weight_output_loss: 0.9720 - bag_output_loss: 0.8285 - footwear_output_loss: 0.7594 - pose_output_loss: 0.5033 - emotion_output_loss: 0.8566 - gender_output_acc: 0.8977 - image_quality_output_acc: 0.5536 - age_output_acc: 0.4057 - weight_output_acc: 0.6345 - bag_output_acc: 0.6307 - footwear_output_acc: 0.6673 - pose_output_acc: 0.7943 - emotion_output_acc: 0.7159 - val_loss: 7.6181 - val_gender_output_loss: 0.4175 - val_image_quality_output_loss: 0.9745 - val_age_output_loss: 1.4211 - val_weight_output_loss: 0.9823 - val_bag_output_loss: 0.8787 - val_footwear_output_loss: 0.8587 - val_pose_output_loss: 0.5796 - val_emotion_output_loss: 0.9189 - val_gender_output_acc: 0.8182 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3635 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5927 - val_footwear_output_acc: 0.5964 - val_pose_output_acc: 0.7542 - val_emotion_output_acc: 0.6880\n",
            "Epoch 44/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 140s 780ms/step - loss: 7.0690 - gender_output_loss: 0.2502 - image_quality_output_loss: 0.9703 - age_output_loss: 1.3826 - weight_output_loss: 0.9712 - bag_output_loss: 0.8275 - footwear_output_loss: 0.7533 - pose_output_loss: 0.4746 - emotion_output_loss: 0.8540 - gender_output_acc: 0.9029 - image_quality_output_acc: 0.5539 - age_output_acc: 0.4094 - weight_output_acc: 0.6346 - bag_output_acc: 0.6319 - footwear_output_acc: 0.6744 - pose_output_acc: 0.8106 - emotion_output_acc: 0.7162 - val_loss: 7.6573 - val_gender_output_loss: 0.4007 - val_image_quality_output_loss: 0.9717 - val_age_output_loss: 1.4210 - val_weight_output_loss: 0.9806 - val_bag_output_loss: 0.8833 - val_footwear_output_loss: 0.7833 - val_pose_output_loss: 0.7050 - val_emotion_output_loss: 0.9280 - val_gender_output_acc: 0.8286 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3661 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6010 - val_footwear_output_acc: 0.6432 - val_pose_output_acc: 0.6885 - val_emotion_output_acc: 0.6880\n",
            "Epoch 45/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 142s 788ms/step - loss: 7.0555 - gender_output_loss: 0.2398 - image_quality_output_loss: 0.9714 - age_output_loss: 1.3812 - weight_output_loss: 0.9708 - bag_output_loss: 0.8234 - footwear_output_loss: 0.7531 - pose_output_loss: 0.4794 - emotion_output_loss: 0.8541 - gender_output_acc: 0.9031 - image_quality_output_acc: 0.5536 - age_output_acc: 0.4089 - weight_output_acc: 0.6346 - bag_output_acc: 0.6363 - footwear_output_acc: 0.6707 - pose_output_acc: 0.8083 - emotion_output_acc: 0.7159 - val_loss: 7.4707 - val_gender_output_loss: 0.4384 - val_image_quality_output_loss: 0.9704 - val_age_output_loss: 1.4194 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.8684 - val_footwear_output_loss: 0.7693 - val_pose_output_loss: 0.5326 - val_emotion_output_loss: 0.9087 - val_gender_output_acc: 0.8380 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3698 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5885 - val_footwear_output_acc: 0.6583 - val_pose_output_acc: 0.8057 - val_emotion_output_acc: 0.6880\n",
            "Epoch 46/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 139s 775ms/step - loss: 7.0348 - gender_output_loss: 0.2409 - image_quality_output_loss: 0.9702 - age_output_loss: 1.3801 - weight_output_loss: 0.9711 - bag_output_loss: 0.8269 - footwear_output_loss: 0.7475 - pose_output_loss: 0.4668 - emotion_output_loss: 0.8520 - gender_output_acc: 0.9030 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4075 - weight_output_acc: 0.6346 - bag_output_acc: 0.6349 - footwear_output_acc: 0.6774 - pose_output_acc: 0.8145 - emotion_output_acc: 0.7163 - val_loss: 7.6792 - val_gender_output_loss: 0.4928 - val_image_quality_output_loss: 0.9729 - val_age_output_loss: 1.4185 - val_weight_output_loss: 0.9818 - val_bag_output_loss: 0.9009 - val_footwear_output_loss: 0.7858 - val_pose_output_loss: 0.6239 - val_emotion_output_loss: 0.9246 - val_gender_output_acc: 0.8036 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5813 - val_footwear_output_acc: 0.6589 - val_pose_output_acc: 0.7479 - val_emotion_output_acc: 0.6880\n",
            "Epoch 47/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 140s 779ms/step - loss: 7.0181 - gender_output_loss: 0.2301 - image_quality_output_loss: 0.9687 - age_output_loss: 1.3825 - weight_output_loss: 0.9710 - bag_output_loss: 0.8254 - footwear_output_loss: 0.7525 - pose_output_loss: 0.4580 - emotion_output_loss: 0.8533 - gender_output_acc: 0.9104 - image_quality_output_acc: 0.5552 - age_output_acc: 0.4071 - weight_output_acc: 0.6346 - bag_output_acc: 0.6331 - footwear_output_acc: 0.6690 - pose_output_acc: 0.8194 - emotion_output_acc: 0.7159 - val_loss: 7.4162 - val_gender_output_loss: 0.3941 - val_image_quality_output_loss: 0.9701 - val_age_output_loss: 1.4093 - val_weight_output_loss: 0.9810 - val_bag_output_loss: 0.8820 - val_footwear_output_loss: 0.7756 - val_pose_output_loss: 0.5199 - val_emotion_output_loss: 0.9090 - val_gender_output_acc: 0.8500 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3724 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6031 - val_footwear_output_acc: 0.6620 - val_pose_output_acc: 0.7927 - val_emotion_output_acc: 0.6880\n",
            "Epoch 48/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 140s 780ms/step - loss: 6.9944 - gender_output_loss: 0.2264 - image_quality_output_loss: 0.9699 - age_output_loss: 1.3807 - weight_output_loss: 0.9723 - bag_output_loss: 0.8232 - footwear_output_loss: 0.7468 - pose_output_loss: 0.4495 - emotion_output_loss: 0.8516 - gender_output_acc: 0.9124 - image_quality_output_acc: 0.5569 - age_output_acc: 0.4086 - weight_output_acc: 0.6346 - bag_output_acc: 0.6366 - footwear_output_acc: 0.6729 - pose_output_acc: 0.8261 - emotion_output_acc: 0.7160 - val_loss: 7.5312 - val_gender_output_loss: 0.4454 - val_image_quality_output_loss: 0.9702 - val_age_output_loss: 1.4203 - val_weight_output_loss: 0.9824 - val_bag_output_loss: 0.8807 - val_footwear_output_loss: 0.7797 - val_pose_output_loss: 0.5615 - val_emotion_output_loss: 0.9184 - val_gender_output_acc: 0.8292 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3646 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5802 - val_footwear_output_acc: 0.6604 - val_pose_output_acc: 0.7771 - val_emotion_output_acc: 0.6880\n",
            "Epoch 49/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 140s 779ms/step - loss: 6.9716 - gender_output_loss: 0.2249 - image_quality_output_loss: 0.9716 - age_output_loss: 1.3772 - weight_output_loss: 0.9700 - bag_output_loss: 0.8226 - footwear_output_loss: 0.7436 - pose_output_loss: 0.4397 - emotion_output_loss: 0.8506 - gender_output_acc: 0.9124 - image_quality_output_acc: 0.5549 - age_output_acc: 0.4097 - weight_output_acc: 0.6345 - bag_output_acc: 0.6337 - footwear_output_acc: 0.6727 - pose_output_acc: 0.8326 - emotion_output_acc: 0.7161 - val_loss: 7.6820 - val_gender_output_loss: 0.4971 - val_image_quality_output_loss: 0.9729 - val_age_output_loss: 1.4237 - val_weight_output_loss: 0.9835 - val_bag_output_loss: 0.8935 - val_footwear_output_loss: 0.8469 - val_pose_output_loss: 0.5788 - val_emotion_output_loss: 0.9154 - val_gender_output_acc: 0.8260 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3719 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5990 - val_footwear_output_acc: 0.6172 - val_pose_output_acc: 0.7781 - val_emotion_output_acc: 0.6880\n",
            "Epoch 50/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 784ms/step - loss: 6.9487 - gender_output_loss: 0.2181 - image_quality_output_loss: 0.9708 - age_output_loss: 1.3800 - weight_output_loss: 0.9699 - bag_output_loss: 0.8235 - footwear_output_loss: 0.7336 - pose_output_loss: 0.4323 - emotion_output_loss: 0.8516 - gender_output_acc: 0.9163 - image_quality_output_acc: 0.5549 - age_output_acc: 0.4089 - weight_output_acc: 0.6346 - bag_output_acc: 0.6338 - footwear_output_acc: 0.6804 - pose_output_acc: 0.8343 - emotion_output_acc: 0.7161 - val_loss: 7.6726 - val_gender_output_loss: 0.3841 - val_image_quality_output_loss: 0.9739 - val_age_output_loss: 1.4214 - val_weight_output_loss: 0.9815 - val_bag_output_loss: 0.8960 - val_footwear_output_loss: 0.8198 - val_pose_output_loss: 0.6992 - val_emotion_output_loss: 0.9290 - val_gender_output_acc: 0.8599 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3740 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5917 - val_footwear_output_acc: 0.6380 - val_pose_output_acc: 0.7292 - val_emotion_output_acc: 0.6880\n",
            "Epoch 51/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 138s 764ms/step - loss: 6.9311 - gender_output_loss: 0.2166 - image_quality_output_loss: 0.9701 - age_output_loss: 1.3812 - weight_output_loss: 0.9696 - bag_output_loss: 0.8213 - footwear_output_loss: 0.7314 - pose_output_loss: 0.4248 - emotion_output_loss: 0.8496 - gender_output_acc: 0.9147 - image_quality_output_acc: 0.5526 - age_output_acc: 0.4070 - weight_output_acc: 0.6350 - bag_output_acc: 0.6338 - footwear_output_acc: 0.6807 - pose_output_acc: 0.8367 - emotion_output_acc: 0.7161 - val_loss: 8.5980 - val_gender_output_loss: 0.4115 - val_image_quality_output_loss: 0.9738 - val_age_output_loss: 1.4193 - val_weight_output_loss: 0.9803 - val_bag_output_loss: 0.9317 - val_footwear_output_loss: 1.1009 - val_pose_output_loss: 1.2346 - val_emotion_output_loss: 0.9807 - val_gender_output_acc: 0.8203 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3667 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5797 - val_footwear_output_acc: 0.5682 - val_pose_output_acc: 0.4958 - val_emotion_output_acc: 0.6880\n",
            "Epoch 52/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 783ms/step - loss: 6.9023 - gender_output_loss: 0.2122 - image_quality_output_loss: 0.9706 - age_output_loss: 1.3788 - weight_output_loss: 0.9699 - bag_output_loss: 0.8177 - footwear_output_loss: 0.7324 - pose_output_loss: 0.4076 - emotion_output_loss: 0.8490 - gender_output_acc: 0.9183 - image_quality_output_acc: 0.5536 - age_output_acc: 0.4089 - weight_output_acc: 0.6346 - bag_output_acc: 0.6351 - footwear_output_acc: 0.6802 - pose_output_acc: 0.8489 - emotion_output_acc: 0.7161 - val_loss: 7.8336 - val_gender_output_loss: 0.5869 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.4138 - val_weight_output_loss: 0.9806 - val_bag_output_loss: 0.8848 - val_footwear_output_loss: 0.9087 - val_pose_output_loss: 0.6060 - val_emotion_output_loss: 0.9166 - val_gender_output_acc: 0.7937 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3745 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6047 - val_footwear_output_acc: 0.6083 - val_pose_output_acc: 0.7620 - val_emotion_output_acc: 0.6880\n",
            "Epoch 53/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 140s 780ms/step - loss: 6.8708 - gender_output_loss: 0.2034 - image_quality_output_loss: 0.9695 - age_output_loss: 1.3758 - weight_output_loss: 0.9692 - bag_output_loss: 0.8188 - footwear_output_loss: 0.7265 - pose_output_loss: 0.3994 - emotion_output_loss: 0.8464 - gender_output_acc: 0.9229 - image_quality_output_acc: 0.5550 - age_output_acc: 0.4076 - weight_output_acc: 0.6346 - bag_output_acc: 0.6358 - footwear_output_acc: 0.6816 - pose_output_acc: 0.8470 - emotion_output_acc: 0.7160 - val_loss: 7.6692 - val_gender_output_loss: 0.3637 - val_image_quality_output_loss: 0.9835 - val_age_output_loss: 1.4137 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.8692 - val_footwear_output_loss: 1.0340 - val_pose_output_loss: 0.5523 - val_emotion_output_loss: 0.9093 - val_gender_output_acc: 0.8573 - val_image_quality_output_acc: 0.5568 - val_age_output_acc: 0.3677 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6042 - val_footwear_output_acc: 0.5734 - val_pose_output_acc: 0.7823 - val_emotion_output_acc: 0.6880\n",
            "Epoch 54/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 782ms/step - loss: 6.8614 - gender_output_loss: 0.1985 - image_quality_output_loss: 0.9696 - age_output_loss: 1.3763 - weight_output_loss: 0.9687 - bag_output_loss: 0.8154 - footwear_output_loss: 0.7247 - pose_output_loss: 0.4000 - emotion_output_loss: 0.8485 - gender_output_acc: 0.9227 - image_quality_output_acc: 0.5547 - age_output_acc: 0.4069 - weight_output_acc: 0.6345 - bag_output_acc: 0.6372 - footwear_output_acc: 0.6828 - pose_output_acc: 0.8483 - emotion_output_acc: 0.7162 - val_loss: 7.4391 - val_gender_output_loss: 0.3866 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.4238 - val_weight_output_loss: 0.9802 - val_bag_output_loss: 0.8743 - val_footwear_output_loss: 0.8247 - val_pose_output_loss: 0.5052 - val_emotion_output_loss: 0.9124 - val_gender_output_acc: 0.8656 - val_image_quality_output_acc: 0.5557 - val_age_output_acc: 0.3641 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5943 - val_footwear_output_acc: 0.6365 - val_pose_output_acc: 0.8083 - val_emotion_output_acc: 0.6880\n",
            "Epoch 55/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 142s 787ms/step - loss: 6.8249 - gender_output_loss: 0.1974 - image_quality_output_loss: 0.9680 - age_output_loss: 1.3746 - weight_output_loss: 0.9693 - bag_output_loss: 0.8164 - footwear_output_loss: 0.7191 - pose_output_loss: 0.3757 - emotion_output_loss: 0.8470 - gender_output_acc: 0.9236 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4118 - weight_output_acc: 0.6346 - bag_output_acc: 0.6376 - footwear_output_acc: 0.6885 - pose_output_acc: 0.8605 - emotion_output_acc: 0.7160 - val_loss: 7.5310 - val_gender_output_loss: 0.4807 - val_image_quality_output_loss: 0.9744 - val_age_output_loss: 1.4150 - val_weight_output_loss: 0.9783 - val_bag_output_loss: 0.8833 - val_footwear_output_loss: 0.8303 - val_pose_output_loss: 0.4987 - val_emotion_output_loss: 0.9138 - val_gender_output_acc: 0.8172 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3771 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5953 - val_footwear_output_acc: 0.6281 - val_pose_output_acc: 0.8135 - val_emotion_output_acc: 0.6880\n",
            "Epoch 56/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 138s 768ms/step - loss: 6.8122 - gender_output_loss: 0.1898 - image_quality_output_loss: 0.9687 - age_output_loss: 1.3771 - weight_output_loss: 0.9684 - bag_output_loss: 0.8157 - footwear_output_loss: 0.7176 - pose_output_loss: 0.3741 - emotion_output_loss: 0.8455 - gender_output_acc: 0.9274 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4104 - weight_output_acc: 0.6350 - bag_output_acc: 0.6372 - footwear_output_acc: 0.6906 - pose_output_acc: 0.8605 - emotion_output_acc: 0.7162 - val_loss: 7.7278 - val_gender_output_loss: 0.4886 - val_image_quality_output_loss: 0.9718 - val_age_output_loss: 1.4190 - val_weight_output_loss: 0.9804 - val_bag_output_loss: 0.8998 - val_footwear_output_loss: 0.8005 - val_pose_output_loss: 0.6720 - val_emotion_output_loss: 0.9413 - val_gender_output_acc: 0.8177 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3672 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6005 - val_footwear_output_acc: 0.6521 - val_pose_output_acc: 0.7302 - val_emotion_output_acc: 0.6880\n",
            "Epoch 57/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 142s 791ms/step - loss: 6.7734 - gender_output_loss: 0.1851 - image_quality_output_loss: 0.9689 - age_output_loss: 1.3717 - weight_output_loss: 0.9685 - bag_output_loss: 0.8116 - footwear_output_loss: 0.7112 - pose_output_loss: 0.3586 - emotion_output_loss: 0.8446 - gender_output_acc: 0.9316 - image_quality_output_acc: 0.5535 - age_output_acc: 0.4113 - weight_output_acc: 0.6349 - bag_output_acc: 0.6417 - footwear_output_acc: 0.6927 - pose_output_acc: 0.8680 - emotion_output_acc: 0.7161 - val_loss: 7.6862 - val_gender_output_loss: 0.3912 - val_image_quality_output_loss: 0.9740 - val_age_output_loss: 1.4155 - val_weight_output_loss: 0.9801 - val_bag_output_loss: 0.8919 - val_footwear_output_loss: 0.8617 - val_pose_output_loss: 0.7068 - val_emotion_output_loss: 0.9126 - val_gender_output_acc: 0.8531 - val_image_quality_output_acc: 0.5536 - val_age_output_acc: 0.3635 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5974 - val_footwear_output_acc: 0.6297 - val_pose_output_acc: 0.7344 - val_emotion_output_acc: 0.6880\n",
            "Epoch 58/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 139s 775ms/step - loss: 6.7658 - gender_output_loss: 0.1810 - image_quality_output_loss: 0.9688 - age_output_loss: 1.3743 - weight_output_loss: 0.9671 - bag_output_loss: 0.8117 - footwear_output_loss: 0.7032 - pose_output_loss: 0.3611 - emotion_output_loss: 0.8471 - gender_output_acc: 0.9321 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4079 - weight_output_acc: 0.6346 - bag_output_acc: 0.6418 - footwear_output_acc: 0.6928 - pose_output_acc: 0.8675 - emotion_output_acc: 0.7161 - val_loss: 7.5294 - val_gender_output_loss: 0.4119 - val_image_quality_output_loss: 0.9748 - val_age_output_loss: 1.4175 - val_weight_output_loss: 0.9780 - val_bag_output_loss: 0.8739 - val_footwear_output_loss: 0.8809 - val_pose_output_loss: 0.5335 - val_emotion_output_loss: 0.9084 - val_gender_output_acc: 0.8453 - val_image_quality_output_acc: 0.5542 - val_age_output_acc: 0.3677 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5896 - val_footwear_output_acc: 0.6323 - val_pose_output_acc: 0.7922 - val_emotion_output_acc: 0.6880\n",
            "Epoch 59/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 139s 775ms/step - loss: 6.7658 - gender_output_loss: 0.1810 - image_quality_output_loss: 0.9688 - age_output_loss: 1.3743 - weight_output_loss: 0.9671 - bag_output_loss: 0.8117 - footwear_output_loss: 0.7032 - pose_output_loss: 0.3611 - emotion_output_loss: 0.8471 - gender_output_acc: 0.9321 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4079 - weight_output_acc: 0.6346 - bag_output_acc: 0.6418 - footwear_output_acc: 0.6928 - pose_output_acc: 0.8675 - emotion_output_acc: 0.7161 - val_loss: 7.5294 - val_gender_output_loss: 0.4119 - val_image_quality_output_loss: 0.9748 - val_age_output_loss: 1.4175 - val_weight_output_loss: 0.9780 - val_bag_output_loss: 0.8739 - val_footwear_output_loss: 0.8809 - val_pose_output_loss: 0.5335 - val_emotion_output_loss: 0.9084 - val_gender_output_acc: 0.8453 - val_image_quality_output_acc: 0.5542 - val_age_output_acc: 0.3677 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5896 - val_footwear_output_acc: 0.6323 - val_pose_output_acc: 0.7922 - val_emotion_output_acc: 0.6880\n",
            "180/180 [==============================] - 140s 780ms/step - loss: 6.7445 - gender_output_loss: 0.1786 - image_quality_output_loss: 0.9691 - age_output_loss: 1.3702 - weight_output_loss: 0.9660 - bag_output_loss: 0.8128 - footwear_output_loss: 0.7109 - pose_output_loss: 0.3426 - emotion_output_loss: 0.8446 - gender_output_acc: 0.9340 - image_quality_output_acc: 0.5543 - age_output_acc: 0.4117 - weight_output_acc: 0.6347 - bag_output_acc: 0.6396 - footwear_output_acc: 0.6900 - pose_output_acc: 0.8755 - emotion_output_acc: 0.7163 - val_loss: 7.4894 - val_gender_output_loss: 0.4183 - val_image_quality_output_loss: 0.9707 - val_age_output_loss: 1.4117 - val_weight_output_loss: 0.9751 - val_bag_output_loss: 0.8625 - val_footwear_output_loss: 0.7914 - val_pose_output_loss: 0.5958 - val_emotion_output_loss: 0.9149 - val_gender_output_acc: 0.8432 - val_image_quality_output_acc: 0.5578 - val_age_output_acc: 0.3630 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6042 - val_footwear_output_acc: 0.6536 - val_pose_output_acc: 0.7948 - val_emotion_output_acc: 0.6880\n",
            "Epoch 60/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 784ms/step - loss: 6.7368 - gender_output_loss: 0.1789 - image_quality_output_loss: 0.9677 - age_output_loss: 1.3695 - weight_output_loss: 0.9643 - bag_output_loss: 0.8116 - footwear_output_loss: 0.7052 - pose_output_loss: 0.3452 - emotion_output_loss: 0.8461 - gender_output_acc: 0.9330 - image_quality_output_acc: 0.5548 - age_output_acc: 0.4099 - weight_output_acc: 0.6352 - bag_output_acc: 0.6405 - footwear_output_acc: 0.6986 - pose_output_acc: 0.8764 - emotion_output_acc: 0.7160 - val_loss: 7.6073 - val_gender_output_loss: 0.4304 - val_image_quality_output_loss: 0.9696 - val_age_output_loss: 1.4138 - val_weight_output_loss: 0.9797 - val_bag_output_loss: 0.8717 - val_footwear_output_loss: 0.8129 - val_pose_output_loss: 0.6739 - val_emotion_output_loss: 0.9080 - val_gender_output_acc: 0.8495 - val_image_quality_output_acc: 0.5557 - val_age_output_acc: 0.3729 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6026 - val_footwear_output_acc: 0.6484 - val_pose_output_acc: 0.7922 - val_emotion_output_acc: 0.6880\n",
            "Epoch 61/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 137s 762ms/step - loss: 6.7042 - gender_output_loss: 0.1743 - image_quality_output_loss: 0.9677 - age_output_loss: 1.3678 - weight_output_loss: 0.9640 - bag_output_loss: 0.8102 - footwear_output_loss: 0.6966 - pose_output_loss: 0.3318 - emotion_output_loss: 0.8451 - gender_output_acc: 0.9346 - image_quality_output_acc: 0.5540 - age_output_acc: 0.4093 - weight_output_acc: 0.6355 - bag_output_acc: 0.6387 - footwear_output_acc: 0.6984 - pose_output_acc: 0.8814 - emotion_output_acc: 0.7161 - val_loss: 7.4807 - val_gender_output_loss: 0.4428 - val_image_quality_output_loss: 0.9726 - val_age_output_loss: 1.4101 - val_weight_output_loss: 0.9749 - val_bag_output_loss: 0.8723 - val_footwear_output_loss: 0.8308 - val_pose_output_loss: 0.5255 - val_emotion_output_loss: 0.9058 - val_gender_output_acc: 0.8510 - val_image_quality_output_acc: 0.5563 - val_age_output_acc: 0.3714 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6094 - val_footwear_output_acc: 0.6448 - val_pose_output_acc: 0.8214 - val_emotion_output_acc: 0.6880\n",
            "Epoch 62/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 784ms/step - loss: 6.7087 - gender_output_loss: 0.1860 - image_quality_output_loss: 0.9692 - age_output_loss: 1.3667 - weight_output_loss: 0.9610 - bag_output_loss: 0.8105 - footwear_output_loss: 0.6951 - pose_output_loss: 0.3294 - emotion_output_loss: 0.8457 - gender_output_acc: 0.9309 - image_quality_output_acc: 0.5515 - age_output_acc: 0.4109 - weight_output_acc: 0.6368 - bag_output_acc: 0.6416 - footwear_output_acc: 0.7015 - pose_output_acc: 0.8815 - emotion_output_acc: 0.7161 - val_loss: 7.6820 - val_gender_output_loss: 0.4211 - val_image_quality_output_loss: 0.9741 - val_age_output_loss: 1.4450 - val_weight_output_loss: 0.9805 - val_bag_output_loss: 0.8634 - val_footwear_output_loss: 0.8138 - val_pose_output_loss: 0.7072 - val_emotion_output_loss: 0.9327 - val_gender_output_acc: 0.8401 - val_image_quality_output_acc: 0.5542 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.6068 - val_footwear_output_acc: 0.6453 - val_pose_output_acc: 0.7365 - val_emotion_output_acc: 0.6880\n",
            "Epoch 63/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 783ms/step - loss: 6.6631 - gender_output_loss: 0.1706 - image_quality_output_loss: 0.9690 - age_output_loss: 1.3628 - weight_output_loss: 0.9583 - bag_output_loss: 0.8104 - footwear_output_loss: 0.6884 - pose_output_loss: 0.3177 - emotion_output_loss: 0.8424 - gender_output_acc: 0.9364 - image_quality_output_acc: 0.5563 - age_output_acc: 0.4116 - weight_output_acc: 0.6364 - bag_output_acc: 0.6427 - footwear_output_acc: 0.7025 - pose_output_acc: 0.8870 - emotion_output_acc: 0.7161 - val_loss: 7.6691 - val_gender_output_loss: 0.4528 - val_image_quality_output_loss: 0.9707 - val_age_output_loss: 1.4057 - val_weight_output_loss: 0.9717 - val_bag_output_loss: 0.8802 - val_footwear_output_loss: 0.8162 - val_pose_output_loss: 0.7166 - val_emotion_output_loss: 0.9123 - val_gender_output_acc: 0.8297 - val_image_quality_output_acc: 0.5583 - val_age_output_acc: 0.3667 - val_weight_output_acc: 0.6417 - val_bag_output_acc: 0.6021 - val_footwear_output_acc: 0.6411 - val_pose_output_acc: 0.7443 - val_emotion_output_acc: 0.6880\n",
            "Epoch 64/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 139s 773ms/step - loss: 6.6398 - gender_output_loss: 0.1639 - image_quality_output_loss: 0.9681 - age_output_loss: 1.3616 - weight_output_loss: 0.9587 - bag_output_loss: 0.8136 - footwear_output_loss: 0.6825 - pose_output_loss: 0.3077 - emotion_output_loss: 0.8416 - gender_output_acc: 0.9422 - image_quality_output_acc: 0.5558 - age_output_acc: 0.4095 - weight_output_acc: 0.6377 - bag_output_acc: 0.6393 - footwear_output_acc: 0.7067 - pose_output_acc: 0.8887 - emotion_output_acc: 0.7160 - val_loss: 8.0627 - val_gender_output_loss: 0.4426 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 1.4177 - val_weight_output_loss: 0.9799 - val_bag_output_loss: 0.8841 - val_footwear_output_loss: 1.1390 - val_pose_output_loss: 0.7622 - val_emotion_output_loss: 0.9138 - val_gender_output_acc: 0.8245 - val_image_quality_output_acc: 0.5531 - val_age_output_acc: 0.3771 - val_weight_output_acc: 0.6385 - val_bag_output_acc: 0.5990 - val_footwear_output_acc: 0.5766 - val_pose_output_acc: 0.7583 - val_emotion_output_acc: 0.6880\n",
            "Epoch 65/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 140s 777ms/step - loss: 6.6421 - gender_output_loss: 0.1687 - image_quality_output_loss: 0.9684 - age_output_loss: 1.3548 - weight_output_loss: 0.9570 - bag_output_loss: 0.8084 - footwear_output_loss: 0.6916 - pose_output_loss: 0.3109 - emotion_output_loss: 0.8410 - gender_output_acc: 0.9375 - image_quality_output_acc: 0.5538 - age_output_acc: 0.4141 - weight_output_acc: 0.6372 - bag_output_acc: 0.6418 - footwear_output_acc: 0.7031 - pose_output_acc: 0.8894 - emotion_output_acc: 0.7158 - val_loss: 7.6001 - val_gender_output_loss: 0.4742 - val_image_quality_output_loss: 0.9726 - val_age_output_loss: 1.4092 - val_weight_output_loss: 0.9649 - val_bag_output_loss: 0.8700 - val_footwear_output_loss: 0.7788 - val_pose_output_loss: 0.6762 - val_emotion_output_loss: 0.9136 - val_gender_output_acc: 0.8328 - val_image_quality_output_acc: 0.5573 - val_age_output_acc: 0.3698 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5979 - val_footwear_output_acc: 0.6698 - val_pose_output_acc: 0.7542 - val_emotion_output_acc: 0.6880\n",
            "Epoch 66/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 140s 777ms/step - loss: 6.6421 - gender_output_loss: 0.1687 - image_quality_output_loss: 0.9684 - age_output_loss: 1.3548 - weight_output_loss: 0.9570 - bag_output_loss: 0.8084 - footwear_output_loss: 0.6916 - pose_output_loss: 0.3109 - emotion_output_loss: 0.8410 - gender_output_acc: 0.9375 - image_quality_output_acc: 0.5538 - age_output_acc: 0.4141 - weight_output_acc: 0.6372 - bag_output_acc: 0.6418 - footwear_output_acc: 0.7031 - pose_output_acc: 0.8894 - emotion_output_acc: 0.7158 - val_loss: 7.6001 - val_gender_output_loss: 0.4742 - val_image_quality_output_loss: 0.9726 - val_age_output_loss: 1.4092 - val_weight_output_loss: 0.9649 - val_bag_output_loss: 0.8700 - val_footwear_output_loss: 0.7788 - val_pose_output_loss: 0.6762 - val_emotion_output_loss: 0.9136 - val_gender_output_acc: 0.8328 - val_image_quality_output_acc: 0.5573 - val_age_output_acc: 0.3698 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5979 - val_footwear_output_acc: 0.6698 - val_pose_output_acc: 0.7542 - val_emotion_output_acc: 0.6880\n",
            "180/180 [==============================] - 137s 761ms/step - loss: 6.6101 - gender_output_loss: 0.1573 - image_quality_output_loss: 0.9685 - age_output_loss: 1.3552 - weight_output_loss: 0.9515 - bag_output_loss: 0.8074 - footwear_output_loss: 0.6811 - pose_output_loss: 0.3066 - emotion_output_loss: 0.8424 - gender_output_acc: 0.9422 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4152 - weight_output_acc: 0.6375 - bag_output_acc: 0.6414 - footwear_output_acc: 0.7063 - pose_output_acc: 0.8927 - emotion_output_acc: 0.7161 - val_loss: 7.8938 - val_gender_output_loss: 0.6809 - val_image_quality_output_loss: 0.9750 - val_age_output_loss: 1.4446 - val_weight_output_loss: 0.9698 - val_bag_output_loss: 0.8970 - val_footwear_output_loss: 0.8805 - val_pose_output_loss: 0.5846 - val_emotion_output_loss: 0.9221 - val_gender_output_acc: 0.7719 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3667 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5667 - val_footwear_output_acc: 0.6188 - val_pose_output_acc: 0.7885 - val_emotion_output_acc: 0.6880\n",
            "Epoch 67/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 784ms/step - loss: 6.5972 - gender_output_loss: 0.1634 - image_quality_output_loss: 0.9682 - age_output_loss: 1.3520 - weight_output_loss: 0.9484 - bag_output_loss: 0.8102 - footwear_output_loss: 0.6791 - pose_output_loss: 0.2950 - emotion_output_loss: 0.8419 - gender_output_acc: 0.9401 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4150 - weight_output_acc: 0.6393 - bag_output_acc: 0.6398 - footwear_output_acc: 0.7079 - pose_output_acc: 0.8951 - emotion_output_acc: 0.7161 - val_loss: 7.4375 - val_gender_output_loss: 0.4056 - val_image_quality_output_loss: 0.9723 - val_age_output_loss: 1.4014 - val_weight_output_loss: 0.9613 - val_bag_output_loss: 0.8692 - val_footwear_output_loss: 0.7995 - val_pose_output_loss: 0.5828 - val_emotion_output_loss: 0.9070 - val_gender_output_acc: 0.8562 - val_image_quality_output_acc: 0.5578 - val_age_output_acc: 0.3724 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5948 - val_footwear_output_acc: 0.6479 - val_pose_output_acc: 0.7990 - val_emotion_output_acc: 0.6880\n",
            "Epoch 68/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 139s 773ms/step - loss: 6.5710 - gender_output_loss: 0.1600 - image_quality_output_loss: 0.9686 - age_output_loss: 1.3447 - weight_output_loss: 0.9477 - bag_output_loss: 0.8083 - footwear_output_loss: 0.6701 - pose_output_loss: 0.2919 - emotion_output_loss: 0.8420 - gender_output_acc: 0.9407 - image_quality_output_acc: 0.5567 - age_output_acc: 0.4161 - weight_output_acc: 0.6385 - bag_output_acc: 0.6408 - footwear_output_acc: 0.7108 - pose_output_acc: 0.9017 - emotion_output_acc: 0.7161 - val_loss: 7.7269 - val_gender_output_loss: 0.4474 - val_image_quality_output_loss: 0.9780 - val_age_output_loss: 1.4739 - val_weight_output_loss: 1.0062 - val_bag_output_loss: 0.8734 - val_footwear_output_loss: 0.8552 - val_pose_output_loss: 0.6338 - val_emotion_output_loss: 0.9216 - val_gender_output_acc: 0.8333 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3448 - val_weight_output_acc: 0.6359 - val_bag_output_acc: 0.5750 - val_footwear_output_acc: 0.6151 - val_pose_output_acc: 0.8068 - val_emotion_output_acc: 0.6880\n",
            "Epoch 69/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 140s 776ms/step - loss: 6.5418 - gender_output_loss: 0.1579 - image_quality_output_loss: 0.9691 - age_output_loss: 1.3405 - weight_output_loss: 0.9415 - bag_output_loss: 0.8086 - footwear_output_loss: 0.6659 - pose_output_loss: 0.2811 - emotion_output_loss: 0.8403 - gender_output_acc: 0.9415 - image_quality_output_acc: 0.5541 - age_output_acc: 0.4181 - weight_output_acc: 0.6391 - bag_output_acc: 0.6397 - footwear_output_acc: 0.7129 - pose_output_acc: 0.9043 - emotion_output_acc: 0.7161 - val_loss: 7.4479 - val_gender_output_loss: 0.4183 - val_image_quality_output_loss: 0.9751 - val_age_output_loss: 1.4156 - val_weight_output_loss: 0.9759 - val_bag_output_loss: 0.8690 - val_footwear_output_loss: 0.8038 - val_pose_output_loss: 0.5386 - val_emotion_output_loss: 0.9152 - val_gender_output_acc: 0.8557 - val_image_quality_output_acc: 0.5542 - val_age_output_acc: 0.3771 - val_weight_output_acc: 0.6464 - val_bag_output_acc: 0.5906 - val_footwear_output_acc: 0.6396 - val_pose_output_acc: 0.8151 - val_emotion_output_acc: 0.6880\n",
            "Epoch 70/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 140s 776ms/step - loss: 6.5418 - gender_output_loss: 0.1579 - image_quality_output_loss: 0.9691 - age_output_loss: 1.3405 - weight_output_loss: 0.9415 - bag_output_loss: 0.8086 - footwear_output_loss: 0.6659 - pose_output_loss: 0.2811 - emotion_output_loss: 0.8403 - gender_output_acc: 0.9415 - image_quality_output_acc: 0.5541 - age_output_acc: 0.4181 - weight_output_acc: 0.6391 - bag_output_acc: 0.6397 - footwear_output_acc: 0.7129 - pose_output_acc: 0.9043 - emotion_output_acc: 0.7161 - val_loss: 7.4479 - val_gender_output_loss: 0.4183 - val_image_quality_output_loss: 0.9751 - val_age_output_loss: 1.4156 - val_weight_output_loss: 0.9759 - val_bag_output_loss: 0.8690 - val_footwear_output_loss: 0.8038 - val_pose_output_loss: 0.5386 - val_emotion_output_loss: 0.9152 - val_gender_output_acc: 0.8557 - val_image_quality_output_acc: 0.5542 - val_age_output_acc: 0.3771 - val_weight_output_acc: 0.6464 - val_bag_output_acc: 0.5906 - val_footwear_output_acc: 0.6396 - val_pose_output_acc: 0.8151 - val_emotion_output_acc: 0.6880\n",
            "180/180 [==============================] - 140s 775ms/step - loss: 6.5146 - gender_output_loss: 0.1497 - image_quality_output_loss: 0.9691 - age_output_loss: 1.3377 - weight_output_loss: 0.9377 - bag_output_loss: 0.8067 - footwear_output_loss: 0.6579 - pose_output_loss: 0.2771 - emotion_output_loss: 0.8427 - gender_output_acc: 0.9446 - image_quality_output_acc: 0.5545 - age_output_acc: 0.4190 - weight_output_acc: 0.6419 - bag_output_acc: 0.6411 - footwear_output_acc: 0.7159 - pose_output_acc: 0.9030 - emotion_output_acc: 0.7162 - val_loss: 7.4945 - val_gender_output_loss: 0.4502 - val_image_quality_output_loss: 0.9763 - val_age_output_loss: 1.4086 - val_weight_output_loss: 0.9624 - val_bag_output_loss: 0.8813 - val_footwear_output_loss: 0.8172 - val_pose_output_loss: 0.5535 - val_emotion_output_loss: 0.9094 - val_gender_output_acc: 0.8401 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.6448 - val_bag_output_acc: 0.5870 - val_footwear_output_acc: 0.6396 - val_pose_output_acc: 0.8099 - val_emotion_output_acc: 0.6880\n",
            "Epoch 71/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 138s 768ms/step - loss: 6.5146 - gender_output_loss: 0.1500 - image_quality_output_loss: 0.9681 - age_output_loss: 1.3347 - weight_output_loss: 0.9378 - bag_output_loss: 0.8084 - footwear_output_loss: 0.6608 - pose_output_loss: 0.2770 - emotion_output_loss: 0.8422 - gender_output_acc: 0.9431 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4204 - weight_output_acc: 0.6419 - bag_output_acc: 0.6430 - footwear_output_acc: 0.7165 - pose_output_acc: 0.9035 - emotion_output_acc: 0.7160 - val_loss: 7.8522 - val_gender_output_loss: 0.5185 - val_image_quality_output_loss: 0.9707 - val_age_output_loss: 1.3992 - val_weight_output_loss: 0.9550 - val_bag_output_loss: 0.8856 - val_footwear_output_loss: 0.8910 - val_pose_output_loss: 0.7627 - val_emotion_output_loss: 0.9341 - val_gender_output_acc: 0.8411 - val_image_quality_output_acc: 0.5557 - val_age_output_acc: 0.3875 - val_weight_output_acc: 0.6422 - val_bag_output_acc: 0.6062 - val_footwear_output_acc: 0.6297 - val_pose_output_acc: 0.7521 - val_emotion_output_acc: 0.6880\n",
            "Epoch 72/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 785ms/step - loss: 6.4889 - gender_output_loss: 0.1576 - image_quality_output_loss: 0.9692 - age_output_loss: 1.3290 - weight_output_loss: 0.9285 - bag_output_loss: 0.8045 - footwear_output_loss: 0.6572 - pose_output_loss: 0.2691 - emotion_output_loss: 0.8389 - gender_output_acc: 0.9425 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4255 - weight_output_acc: 0.6416 - bag_output_acc: 0.6422 - footwear_output_acc: 0.7168 - pose_output_acc: 0.9090 - emotion_output_acc: 0.7160 - val_loss: 8.1211 - val_gender_output_loss: 0.4792 - val_image_quality_output_loss: 0.9761 - val_age_output_loss: 1.4320 - val_weight_output_loss: 0.9927 - val_bag_output_loss: 0.9054 - val_footwear_output_loss: 0.8188 - val_pose_output_loss: 1.0713 - val_emotion_output_loss: 0.9110 - val_gender_output_acc: 0.8406 - val_image_quality_output_acc: 0.5531 - val_age_output_acc: 0.3797 - val_weight_output_acc: 0.6323 - val_bag_output_acc: 0.5927 - val_footwear_output_acc: 0.6609 - val_pose_output_acc: 0.7063 - val_emotion_output_acc: 0.6880\n",
            "Epoch 73/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 142s 788ms/step - loss: 6.4868 - gender_output_loss: 0.1549 - image_quality_output_loss: 0.9691 - age_output_loss: 1.3291 - weight_output_loss: 0.9251 - bag_output_loss: 0.8090 - footwear_output_loss: 0.6506 - pose_output_loss: 0.2752 - emotion_output_loss: 0.8393 - gender_output_acc: 0.9444 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4227 - weight_output_acc: 0.6429 - bag_output_acc: 0.6421 - footwear_output_acc: 0.7186 - pose_output_acc: 0.9052 - emotion_output_acc: 0.7161 - val_loss: 8.2002 - val_gender_output_loss: 0.4134 - val_image_quality_output_loss: 0.9742 - val_age_output_loss: 1.3930 - val_weight_output_loss: 0.9663 - val_bag_output_loss: 0.8881 - val_footwear_output_loss: 0.8152 - val_pose_output_loss: 1.3006 - val_emotion_output_loss: 0.9151 - val_gender_output_acc: 0.8432 - val_image_quality_output_acc: 0.5557 - val_age_output_acc: 0.3776 - val_weight_output_acc: 0.6474 - val_bag_output_acc: 0.5922 - val_footwear_output_acc: 0.6448 - val_pose_output_acc: 0.5859 - val_emotion_output_acc: 0.6880\n",
            "Epoch 74/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 786ms/step - loss: 6.4293 - gender_output_loss: 0.1414 - image_quality_output_loss: 0.9674 - age_output_loss: 1.3207 - weight_output_loss: 0.9241 - bag_output_loss: 0.8009 - footwear_output_loss: 0.6457 - pose_output_loss: 0.2573 - emotion_output_loss: 0.8378 - gender_output_acc: 0.9482 - image_quality_output_acc: 0.5541 - age_output_acc: 0.4265 - weight_output_acc: 0.6437 - bag_output_acc: 0.6477 - footwear_output_acc: 0.7217 - pose_output_acc: 0.9135 - emotion_output_acc: 0.7161 - val_loss: 7.4432 - val_gender_output_loss: 0.4176 - val_image_quality_output_loss: 0.9725 - val_age_output_loss: 1.3911 - val_weight_output_loss: 0.9553 - val_bag_output_loss: 0.8793 - val_footwear_output_loss: 0.8473 - val_pose_output_loss: 0.5425 - val_emotion_output_loss: 0.9039 - val_gender_output_acc: 0.8708 - val_image_quality_output_acc: 0.5568 - val_age_output_acc: 0.3807 - val_weight_output_acc: 0.6385 - val_bag_output_acc: 0.5948 - val_footwear_output_acc: 0.6464 - val_pose_output_acc: 0.8203 - val_emotion_output_acc: 0.6880\n",
            "Epoch 75/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 141s 784ms/step - loss: 6.4046 - gender_output_loss: 0.1379 - image_quality_output_loss: 0.9689 - age_output_loss: 1.3157 - weight_output_loss: 0.9159 - bag_output_loss: 0.7997 - footwear_output_loss: 0.6495 - pose_output_loss: 0.2456 - emotion_output_loss: 0.8380 - gender_output_acc: 0.9508 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4279 - weight_output_acc: 0.6471 - bag_output_acc: 0.6455 - footwear_output_acc: 0.7201 - pose_output_acc: 0.9166 - emotion_output_acc: 0.7161 - val_loss: 7.7527 - val_gender_output_loss: 0.4232 - val_image_quality_output_loss: 0.9721 - val_age_output_loss: 1.3940 - val_weight_output_loss: 0.9497 - val_bag_output_loss: 0.8720 - val_footwear_output_loss: 0.8161 - val_pose_output_loss: 0.8471 - val_emotion_output_loss: 0.9453 - val_gender_output_acc: 0.8646 - val_image_quality_output_acc: 0.5547 - val_age_output_acc: 0.3865 - val_weight_output_acc: 0.6427 - val_bag_output_acc: 0.6047 - val_footwear_output_acc: 0.6479 - val_pose_output_acc: 0.7302 - val_emotion_output_acc: 0.6880\n",
            "Epoch 76/100\n",
            "Learning rate:  0.13999999999999999\n",
            "180/180 [==============================] - 140s 778ms/step - loss: 6.3983 - gender_output_loss: 0.1394 - image_quality_output_loss: 0.9684 - age_output_loss: 1.3141 - weight_output_loss: 0.9124 - bag_output_loss: 0.7988 - footwear_output_loss: 0.6475 - pose_output_loss: 0.2460 - emotion_output_loss: 0.8387 - gender_output_acc: 0.9516 - image_quality_output_acc: 0.5546 - age_output_acc: 0.4257 - weight_output_acc: 0.6488 - bag_output_acc: 0.6482 - footwear_output_acc: 0.7202 - pose_output_acc: 0.9169 - emotion_output_acc: 0.7164 - val_loss: 7.9790 - val_gender_output_loss: 0.5260 - val_image_quality_output_loss: 0.9827 - val_age_output_loss: 1.4136 - val_weight_output_loss: 0.9541 - val_bag_output_loss: 0.8782 - val_footwear_output_loss: 1.1694 - val_pose_output_loss: 0.6093 - val_emotion_output_loss: 0.9126 - val_gender_output_acc: 0.8089 - val_image_quality_output_acc: 0.5531 - val_age_output_acc: 0.3766 - val_weight_output_acc: 0.6469 - val_bag_output_acc: 0.5875 - val_footwear_output_acc: 0.5849 - val_pose_output_acc: 0.7937 - val_emotion_output_acc: 0.6880\n",
            "Epoch 77/100\n",
            "Learning rate:  0.13999999999999999\n",
            "132/180 [=====================>........] - ETA: 37s - loss: 6.3665 - gender_output_loss: 0.1436 - image_quality_output_loss: 0.9716 - age_output_loss: 1.3082 - weight_output_loss: 0.9165 - bag_output_loss: 0.8007 - footwear_output_loss: 0.6341 - pose_output_loss: 0.2293 - emotion_output_loss: 0.8297 - gender_output_acc: 0.9482 - image_quality_output_acc: 0.5485 - age_output_acc: 0.4297 - weight_output_acc: 0.6468 - bag_output_acc: 0.6474 - footwear_output_acc: 0.7282 - pose_output_acc: 0.9227 - emotion_output_acc: 0.7185"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:610: UserWarning: The input 30 could not be retrieved. It could be because a worker has died.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "154/180 [========================>.....] - ETA: 24s - loss: 6.3592 - gender_output_loss: 0.1443 - image_quality_output_loss: 0.9701 - age_output_loss: 1.3058 - weight_output_loss: 0.9116 - bag_output_loss: 0.8001 - footwear_output_loss: 0.6312 - pose_output_loss: 0.2298 - emotion_output_loss: 0.8334 - gender_output_acc: 0.9473 - image_quality_output_acc: 0.5501 - age_output_acc: 0.4316 - weight_output_acc: 0.6507 - bag_output_acc: 0.6475 - footwear_output_acc: 0.7285 - pose_output_acc: 0.9220 - emotion_output_acc: 0.7169"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:610: UserWarning: The input 16 could not be retrieved. It could be because a worker has died.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "172/180 [===========================>..] - ETA: 8s - loss: 6.3640 - gender_output_loss: 0.1412 - image_quality_output_loss: 0.9686 - age_output_loss: 1.3063 - weight_output_loss: 0.9109 - bag_output_loss: 0.8001 - footwear_output_loss: 0.6336 - pose_output_loss: 0.2329 - emotion_output_loss: 0.8375 - gender_output_acc: 0.9489 - image_quality_output_acc: 0.5515 - age_output_acc: 0.4306 - weight_output_acc: 0.6503 - bag_output_acc: 0.6459 - footwear_output_acc: 0.7277 - pose_output_acc: 0.9214 - emotion_output_acc: 0.7159"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:610: UserWarning: The input 62 could not be retrieved. It could be because a worker has died.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "180/180 [==============================] - 227s 1s/step - loss: 6.3617 - gender_output_loss: 0.1396 - image_quality_output_loss: 0.9678 - age_output_loss: 1.3090 - weight_output_loss: 0.9111 - bag_output_loss: 0.8002 - footwear_output_loss: 0.6322 - pose_output_loss: 0.2325 - emotion_output_loss: 0.8364 - gender_output_acc: 0.9493 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4284 - weight_output_acc: 0.6495 - bag_output_acc: 0.6455 - footwear_output_acc: 0.7282 - pose_output_acc: 0.9214 - emotion_output_acc: 0.7163 - val_loss: 8.1226 - val_gender_output_loss: 0.4276 - val_image_quality_output_loss: 0.9784 - val_age_output_loss: 1.4698 - val_weight_output_loss: 0.9825 - val_bag_output_loss: 0.8760 - val_footwear_output_loss: 0.9125 - val_pose_output_loss: 0.9838 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.8578 - val_image_quality_output_acc: 0.5557 - val_age_output_acc: 0.3740 - val_weight_output_acc: 0.6432 - val_bag_output_acc: 0.5953 - val_footwear_output_acc: 0.6135 - val_pose_output_acc: 0.6891 - val_emotion_output_acc: 0.6880\n",
            "Epoch 78/100\n",
            "Learning rate:  0.13999999999999999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-927:\n",
            "Process ForkPoolWorker-930:\n",
            "Process ForkPoolWorker-925:\n",
            "Process ForkPoolWorker-928:\n",
            "Process ForkPoolWorker-926:\n",
            "Process ForkPoolWorker-929:\n",
            "Process ForkPoolWorker-932:\n",
            "Process ForkPoolWorker-935:\n",
            "Process ForkPoolWorker-917:\n",
            "Process ForkPoolWorker-933:\n",
            "Process ForkPoolWorker-934:\n",
            "Process ForkPoolWorker-931:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c7ea406dc13c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTypU4iP8xmS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "da239065-c2cb-4ed9-a6e2-5f98293b9789"
      },
      "source": [
        "Score = model.evaluate_generator(valid_gen)\n",
        "print([[a, b] for a,b in zip(df.columns,Score[9:])])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['gender', 0.8578125], ['imagequality', 0.5557291666666667], ['age', 0.37395833333333334], ['weight', 0.6432291666666666], ['carryingbag', 0.5953125], ['footwear', 0.6135416666666667], ['emotion', 0.6890625], ['bodypose', 0.6880208333333333]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbYidYftJPx0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c278aa6-5c6e-48a9-d782-3cc91f751339"
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    lr = 0.01\n",
        "    if epoch > 60:\n",
        "        lr *= 1e-6\n",
        "    elif epoch > 30:\n",
        "        lr *= 1e-5\n",
        "    elif epoch > 25:\n",
        "        lr *= 1e-4\n",
        "    elif epoch > 15:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 10:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 8:\n",
        "        lr *= 0.5e-1\n",
        "    elif epoch > 5:\n",
        "        lr *= 0.5\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "callbacks = [lr_scheduler, model_save]\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "opt = SGD(lr=0.1)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    # loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "######################  FIT  #######################################\n",
        "model_info = model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    callbacks = callbacks\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "Learning rate:  0.01\n",
            "180/180 [==============================] - 163s 907ms/step - loss: 6.1422 - gender_output_loss: 0.0924 - image_quality_output_loss: 0.9667 - age_output_loss: 1.2750 - weight_output_loss: 0.8876 - bag_output_loss: 0.7932 - footwear_output_loss: 0.5826 - pose_output_loss: 0.1794 - emotion_output_loss: 0.8328 - gender_output_acc: 0.9698 - image_quality_output_acc: 0.5554 - age_output_acc: 0.4401 - weight_output_acc: 0.6578 - bag_output_acc: 0.6445 - footwear_output_acc: 0.7476 - pose_output_acc: 0.9441 - emotion_output_acc: 0.7163 - val_loss: 7.2837 - val_gender_output_loss: 0.3653 - val_image_quality_output_loss: 0.9720 - val_age_output_loss: 1.3922 - val_weight_output_loss: 0.9369 - val_bag_output_loss: 0.8558 - val_footwear_output_loss: 0.8282 - val_pose_output_loss: 0.5006 - val_emotion_output_loss: 0.9003 - val_gender_output_acc: 0.8802 - val_image_quality_output_acc: 0.5573 - val_age_output_acc: 0.3839 - val_weight_output_acc: 0.6427 - val_bag_output_acc: 0.5979 - val_footwear_output_acc: 0.6505 - val_pose_output_acc: 0.8422 - val_emotion_output_acc: 0.6880\n",
            "Epoch 2/100\n",
            "Learning rate:  0.01\n",
            "180/180 [==============================] - 146s 812ms/step - loss: 6.0806 - gender_output_loss: 0.0848 - image_quality_output_loss: 0.9675 - age_output_loss: 1.2691 - weight_output_loss: 0.8793 - bag_output_loss: 0.7872 - footwear_output_loss: 0.5712 - pose_output_loss: 0.1566 - emotion_output_loss: 0.8328 - gender_output_acc: 0.9714 - image_quality_output_acc: 0.5540 - age_output_acc: 0.4419 - weight_output_acc: 0.6612 - bag_output_acc: 0.6537 - footwear_output_acc: 0.7550 - pose_output_acc: 0.9543 - emotion_output_acc: 0.7159 - val_loss: 7.3053 - val_gender_output_loss: 0.3740 - val_image_quality_output_loss: 0.9727 - val_age_output_loss: 1.3907 - val_weight_output_loss: 0.9358 - val_bag_output_loss: 0.8567 - val_footwear_output_loss: 0.8335 - val_pose_output_loss: 0.5082 - val_emotion_output_loss: 0.9017 - val_gender_output_acc: 0.8818 - val_image_quality_output_acc: 0.5578 - val_age_output_acc: 0.3818 - val_weight_output_acc: 0.6432 - val_bag_output_acc: 0.6068 - val_footwear_output_acc: 0.6516 - val_pose_output_acc: 0.8458 - val_emotion_output_acc: 0.6880\n",
            "Epoch 3/100\n",
            "Learning rate:  0.01\n",
            "180/180 [==============================] - 146s 810ms/step - loss: 6.0383 - gender_output_loss: 0.0807 - image_quality_output_loss: 0.9663 - age_output_loss: 1.2626 - weight_output_loss: 0.8803 - bag_output_loss: 0.7834 - footwear_output_loss: 0.5527 - pose_output_loss: 0.1507 - emotion_output_loss: 0.8299 - gender_output_acc: 0.9727 - image_quality_output_acc: 0.5549 - age_output_acc: 0.4424 - weight_output_acc: 0.6570 - bag_output_acc: 0.6567 - footwear_output_acc: 0.7628 - pose_output_acc: 0.9571 - emotion_output_acc: 0.7160 - val_loss: 7.3149 - val_gender_output_loss: 0.3768 - val_image_quality_output_loss: 0.9739 - val_age_output_loss: 1.3849 - val_weight_output_loss: 0.9338 - val_bag_output_loss: 0.8563 - val_footwear_output_loss: 0.8374 - val_pose_output_loss: 0.5185 - val_emotion_output_loss: 0.9017 - val_gender_output_acc: 0.8781 - val_image_quality_output_acc: 0.5552 - val_age_output_acc: 0.3812 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.6036 - val_footwear_output_acc: 0.6557 - val_pose_output_acc: 0.8380 - val_emotion_output_acc: 0.6880\n",
            "Epoch 4/100\n",
            "Learning rate:  0.01\n",
            "180/180 [==============================] - 138s 768ms/step - loss: 6.0205 - gender_output_loss: 0.0792 - image_quality_output_loss: 0.9670 - age_output_loss: 1.2590 - weight_output_loss: 0.8809 - bag_output_loss: 0.7797 - footwear_output_loss: 0.5500 - pose_output_loss: 0.1416 - emotion_output_loss: 0.8317 - gender_output_acc: 0.9772 - image_quality_output_acc: 0.5548 - age_output_acc: 0.4468 - weight_output_acc: 0.6577 - bag_output_acc: 0.6595 - footwear_output_acc: 0.7635 - pose_output_acc: 0.9614 - emotion_output_acc: 0.7161 - val_loss: 7.3695 - val_gender_output_loss: 0.3823 - val_image_quality_output_loss: 0.9731 - val_age_output_loss: 1.4049 - val_weight_output_loss: 0.9408 - val_bag_output_loss: 0.8587 - val_footwear_output_loss: 0.8554 - val_pose_output_loss: 0.5181 - val_emotion_output_loss: 0.9050 - val_gender_output_acc: 0.8786 - val_image_quality_output_acc: 0.5568 - val_age_output_acc: 0.3839 - val_weight_output_acc: 0.6422 - val_bag_output_acc: 0.6052 - val_footwear_output_acc: 0.6500 - val_pose_output_acc: 0.8365 - val_emotion_output_acc: 0.6880\n",
            "Epoch 5/100\n",
            "Learning rate:  0.01\n",
            "180/180 [==============================] - 145s 808ms/step - loss: 5.9955 - gender_output_loss: 0.0788 - image_quality_output_loss: 0.9664 - age_output_loss: 1.2520 - weight_output_loss: 0.8729 - bag_output_loss: 0.7794 - footwear_output_loss: 0.5470 - pose_output_loss: 0.1401 - emotion_output_loss: 0.8280 - gender_output_acc: 0.9753 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4451 - weight_output_acc: 0.6638 - bag_output_acc: 0.6581 - footwear_output_acc: 0.7675 - pose_output_acc: 0.9600 - emotion_output_acc: 0.7161 - val_loss: 7.3573 - val_gender_output_loss: 0.3895 - val_image_quality_output_loss: 0.9731 - val_age_output_loss: 1.3858 - val_weight_output_loss: 0.9335 - val_bag_output_loss: 0.8614 - val_footwear_output_loss: 0.8583 - val_pose_output_loss: 0.5215 - val_emotion_output_loss: 0.9033 - val_gender_output_acc: 0.8776 - val_image_quality_output_acc: 0.5563 - val_age_output_acc: 0.3818 - val_weight_output_acc: 0.6432 - val_bag_output_acc: 0.6016 - val_footwear_output_acc: 0.6547 - val_pose_output_acc: 0.8344 - val_emotion_output_acc: 0.6880\n",
            "Epoch 6/100\n",
            "Learning rate:  0.01\n",
            "180/180 [==============================] - 144s 798ms/step - loss: 5.9828 - gender_output_loss: 0.0755 - image_quality_output_loss: 0.9667 - age_output_loss: 1.2527 - weight_output_loss: 0.8706 - bag_output_loss: 0.7804 - footwear_output_loss: 0.5382 - pose_output_loss: 0.1380 - emotion_output_loss: 0.8301 - gender_output_acc: 0.9769 - image_quality_output_acc: 0.5543 - age_output_acc: 0.4489 - weight_output_acc: 0.6641 - bag_output_acc: 0.6577 - footwear_output_acc: 0.7713 - pose_output_acc: 0.9610 - emotion_output_acc: 0.7162 - val_loss: 7.3634 - val_gender_output_loss: 0.3806 - val_image_quality_output_loss: 0.9726 - val_age_output_loss: 1.3873 - val_weight_output_loss: 0.9314 - val_bag_output_loss: 0.8580 - val_footwear_output_loss: 0.8546 - val_pose_output_loss: 0.5435 - val_emotion_output_loss: 0.9049 - val_gender_output_acc: 0.8818 - val_image_quality_output_acc: 0.5578 - val_age_output_acc: 0.3870 - val_weight_output_acc: 0.6438 - val_bag_output_acc: 0.6010 - val_footwear_output_acc: 0.6552 - val_pose_output_acc: 0.8391 - val_emotion_output_acc: 0.6880\n",
            "Epoch 7/100\n",
            "Learning rate:  0.005\n",
            "180/180 [==============================] - 143s 797ms/step - loss: 5.9627 - gender_output_loss: 0.0722 - image_quality_output_loss: 0.9655 - age_output_loss: 1.2494 - weight_output_loss: 0.8684 - bag_output_loss: 0.7800 - footwear_output_loss: 0.5346 - pose_output_loss: 0.1339 - emotion_output_loss: 0.8284 - gender_output_acc: 0.9772 - image_quality_output_acc: 0.5559 - age_output_acc: 0.4483 - weight_output_acc: 0.6615 - bag_output_acc: 0.6564 - footwear_output_acc: 0.7682 - pose_output_acc: 0.9618 - emotion_output_acc: 0.7161 - val_loss: 7.3802 - val_gender_output_loss: 0.3920 - val_image_quality_output_loss: 0.9732 - val_age_output_loss: 1.3954 - val_weight_output_loss: 0.9343 - val_bag_output_loss: 0.8598 - val_footwear_output_loss: 0.8590 - val_pose_output_loss: 0.5310 - val_emotion_output_loss: 0.9053 - val_gender_output_acc: 0.8781 - val_image_quality_output_acc: 0.5573 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.6438 - val_bag_output_acc: 0.6021 - val_footwear_output_acc: 0.6495 - val_pose_output_acc: 0.8396 - val_emotion_output_acc: 0.6880\n",
            "Epoch 8/100\n",
            "Learning rate:  0.005\n",
            "180/180 [==============================] - 143s 797ms/step - loss: 5.9627 - gender_output_loss: 0.0722 - image_quality_output_loss: 0.9655 - age_output_loss: 1.2494 - weight_output_loss: 0.8684 - bag_output_loss: 0.7800 - footwear_output_loss: 0.5346 - pose_output_loss: 0.1339 - emotion_output_loss: 0.8284 - gender_output_acc: 0.9772 - image_quality_output_acc: 0.5559 - age_output_acc: 0.4483 - weight_output_acc: 0.6615 - bag_output_acc: 0.6564 - footwear_output_acc: 0.7682 - pose_output_acc: 0.9618 - emotion_output_acc: 0.7161 - val_loss: 7.3802 - val_gender_output_loss: 0.3920 - val_image_quality_output_loss: 0.9732 - val_age_output_loss: 1.3954 - val_weight_output_loss: 0.9343 - val_bag_output_loss: 0.8598 - val_footwear_output_loss: 0.8590 - val_pose_output_loss: 0.5310 - val_emotion_output_loss: 0.9053 - val_gender_output_acc: 0.8781 - val_image_quality_output_acc: 0.5573 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.6438 - val_bag_output_acc: 0.6021 - val_footwear_output_acc: 0.6495 - val_pose_output_acc: 0.8396 - val_emotion_output_acc: 0.6880\n",
            "180/180 [==============================] - 144s 801ms/step - loss: 5.9498 - gender_output_loss: 0.0696 - image_quality_output_loss: 0.9660 - age_output_loss: 1.2467 - weight_output_loss: 0.8681 - bag_output_loss: 0.7762 - footwear_output_loss: 0.5330 - pose_output_loss: 0.1293 - emotion_output_loss: 0.8306 - gender_output_acc: 0.9782 - image_quality_output_acc: 0.5536 - age_output_acc: 0.4512 - weight_output_acc: 0.6627 - bag_output_acc: 0.6610 - footwear_output_acc: 0.7716 - pose_output_acc: 0.9637 - emotion_output_acc: 0.7159 - val_loss: 7.3908 - val_gender_output_loss: 0.4061 - val_image_quality_output_loss: 0.9732 - val_age_output_loss: 1.3913 - val_weight_output_loss: 0.9326 - val_bag_output_loss: 0.8609 - val_footwear_output_loss: 0.8672 - val_pose_output_loss: 0.5256 - val_emotion_output_loss: 0.9038 - val_gender_output_acc: 0.8792 - val_image_quality_output_acc: 0.5578 - val_age_output_acc: 0.3854 - val_weight_output_acc: 0.6427 - val_bag_output_acc: 0.6000 - val_footwear_output_acc: 0.6490 - val_pose_output_acc: 0.8432 - val_emotion_output_acc: 0.6880\n",
            "Epoch 9/100\n",
            "Learning rate:  0.005\n",
            "180/180 [==============================] - 141s 782ms/step - loss: 5.9545 - gender_output_loss: 0.0753 - image_quality_output_loss: 0.9662 - age_output_loss: 1.2460 - weight_output_loss: 0.8648 - bag_output_loss: 0.7788 - footwear_output_loss: 0.5342 - pose_output_loss: 0.1305 - emotion_output_loss: 0.8288 - gender_output_acc: 0.9769 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4502 - weight_output_acc: 0.6634 - bag_output_acc: 0.6588 - footwear_output_acc: 0.7728 - pose_output_acc: 0.9643 - emotion_output_acc: 0.7162 - val_loss: 7.3973 - val_gender_output_loss: 0.3962 - val_image_quality_output_loss: 0.9737 - val_age_output_loss: 1.3896 - val_weight_output_loss: 0.9330 - val_bag_output_loss: 0.8628 - val_footwear_output_loss: 0.8653 - val_pose_output_loss: 0.5433 - val_emotion_output_loss: 0.9036 - val_gender_output_acc: 0.8786 - val_image_quality_output_acc: 0.5573 - val_age_output_acc: 0.3818 - val_weight_output_acc: 0.6448 - val_bag_output_acc: 0.5974 - val_footwear_output_acc: 0.6516 - val_pose_output_acc: 0.8406 - val_emotion_output_acc: 0.6880\n",
            "Epoch 10/100\n",
            "Learning rate:  0.0005\n",
            "180/180 [==============================] - 142s 787ms/step - loss: 5.9401 - gender_output_loss: 0.0711 - image_quality_output_loss: 0.9675 - age_output_loss: 1.2385 - weight_output_loss: 0.8646 - bag_output_loss: 0.7767 - footwear_output_loss: 0.5298 - pose_output_loss: 0.1332 - emotion_output_loss: 0.8289 - gender_output_acc: 0.9782 - image_quality_output_acc: 0.5534 - age_output_acc: 0.4511 - weight_output_acc: 0.6655 - bag_output_acc: 0.6606 - footwear_output_acc: 0.7711 - pose_output_acc: 0.9613 - emotion_output_acc: 0.7161 - val_loss: 7.3999 - val_gender_output_loss: 0.3990 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.3924 - val_weight_output_loss: 0.9329 - val_bag_output_loss: 0.8622 - val_footwear_output_loss: 0.8654 - val_pose_output_loss: 0.5409 - val_emotion_output_loss: 0.9039 - val_gender_output_acc: 0.8807 - val_image_quality_output_acc: 0.5589 - val_age_output_acc: 0.3880 - val_weight_output_acc: 0.6464 - val_bag_output_acc: 0.5984 - val_footwear_output_acc: 0.6510 - val_pose_output_acc: 0.8396 - val_emotion_output_acc: 0.6880\n",
            "Epoch 11/100\n",
            "Learning rate:  0.0005\n",
            "180/180 [==============================] - 145s 805ms/step - loss: 5.9368 - gender_output_loss: 0.0673 - image_quality_output_loss: 0.9669 - age_output_loss: 1.2435 - weight_output_loss: 0.8619 - bag_output_loss: 0.7775 - footwear_output_loss: 0.5302 - pose_output_loss: 0.1323 - emotion_output_loss: 0.8273 - gender_output_acc: 0.9799 - image_quality_output_acc: 0.5536 - age_output_acc: 0.4454 - weight_output_acc: 0.6688 - bag_output_acc: 0.6580 - footwear_output_acc: 0.7753 - pose_output_acc: 0.9622 - emotion_output_acc: 0.7163 - val_loss: 7.4005 - val_gender_output_loss: 0.3984 - val_image_quality_output_loss: 0.9733 - val_age_output_loss: 1.3930 - val_weight_output_loss: 0.9332 - val_bag_output_loss: 0.8621 - val_footwear_output_loss: 0.8653 - val_pose_output_loss: 0.5412 - val_emotion_output_loss: 0.9040 - val_gender_output_acc: 0.8812 - val_image_quality_output_acc: 0.5583 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.6453 - val_bag_output_acc: 0.5984 - val_footwear_output_acc: 0.6505 - val_pose_output_acc: 0.8406 - val_emotion_output_acc: 0.6880\n",
            "Epoch 12/100\n",
            "Learning rate:  0.0001\n",
            "180/180 [==============================] - 143s 793ms/step - loss: 5.9360 - gender_output_loss: 0.0710 - image_quality_output_loss: 0.9659 - age_output_loss: 1.2452 - weight_output_loss: 0.8623 - bag_output_loss: 0.7770 - footwear_output_loss: 0.5291 - pose_output_loss: 0.1299 - emotion_output_loss: 0.8257 - gender_output_acc: 0.9778 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4464 - weight_output_acc: 0.6636 - bag_output_acc: 0.6617 - footwear_output_acc: 0.7747 - pose_output_acc: 0.9618 - emotion_output_acc: 0.7166 - val_loss: 7.4012 - val_gender_output_loss: 0.3987 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.3929 - val_weight_output_loss: 0.9331 - val_bag_output_loss: 0.8622 - val_footwear_output_loss: 0.8654 - val_pose_output_loss: 0.5416 - val_emotion_output_loss: 0.9040 - val_gender_output_acc: 0.8812 - val_image_quality_output_acc: 0.5578 - val_age_output_acc: 0.3865 - val_weight_output_acc: 0.6458 - val_bag_output_acc: 0.5969 - val_footwear_output_acc: 0.6516 - val_pose_output_acc: 0.8411 - val_emotion_output_acc: 0.6880\n",
            "Epoch 13/100\n",
            "Learning rate:  0.0001\n",
            "179/180 [============================>.] - ETA: 0s - loss: 5.9397 - gender_output_loss: 0.0706 - image_quality_output_loss: 0.9659 - age_output_loss: 1.2429 - weight_output_loss: 0.8646 - bag_output_loss: 0.7779 - footwear_output_loss: 0.5277 - pose_output_loss: 0.1311 - emotion_output_loss: 0.8292 - gender_output_acc: 0.9785 - image_quality_output_acc: 0.5543 - age_output_acc: 0.4503 - weight_output_acc: 0.6638 - bag_output_acc: 0.6572 - footwear_output_acc: 0.7751 - pose_output_acc: 0.9630 - emotion_output_acc: 0.7157 - 143s 793ms/step - loss: 5.9360 - gender_output_loss: 0.0710 - image_quality_output_loss: 0.9659 - age_output_loss: 1.2452 - weight_output_loss: 0.8623 - bag_output_loss: 0.7770 - footwear_output_loss: 0.5291 - pose_output_loss: 0.1299 - emotion_output_loss: 0.8257 - gender_output_acc: 0.9778 - image_quality_output_acc: 0.5544 - age_output_acc: 0.4464 - weight_output_acc: 0.6636 - bag_output_acc: 0.6617 - footwear_output_acc: 0.7747 - pose_output_acc: 0.9618 - emotion_output_acc: 0.7166 - val_loss: 7.4012 - val_gender_output_loss: 0.3987 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.3929 - val_weight_output_loss: 0.9331 - val_bag_output_loss: 0.8622 - val_footwear_output_loss: 0.8654 - val_pose_output_loss: 0.5416 - val_emotion_output_loss: 0.9040 - val_gender_output_acc: 0.8812 - val_image_quality_output_acc: 0.5578 - val_age_output_acc: 0.3865 - val_weight_output_acc: 0.6458 - val_bag_output_acc: 0.5969 - val_footwear_output_acc: 0.6516 - val_pose_output_acc: 0.8411 - val_emotion_output_acc: 0.6880\n",
            "180/180 [==============================] - 143s 793ms/step - loss: 5.9393 - gender_output_loss: 0.0707 - image_quality_output_loss: 0.9659 - age_output_loss: 1.2424 - weight_output_loss: 0.8649 - bag_output_loss: 0.7779 - footwear_output_loss: 0.5277 - pose_output_loss: 0.1310 - emotion_output_loss: 0.8290 - gender_output_acc: 0.9786 - image_quality_output_acc: 0.5542 - age_output_acc: 0.4504 - weight_output_acc: 0.6635 - bag_output_acc: 0.6573 - footwear_output_acc: 0.7749 - pose_output_acc: 0.9631 - emotion_output_acc: 0.7159 - val_loss: 7.3993 - val_gender_output_loss: 0.3985 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.3927 - val_weight_output_loss: 0.9329 - val_bag_output_loss: 0.8620 - val_footwear_output_loss: 0.8652 - val_pose_output_loss: 0.5408 - val_emotion_output_loss: 0.9039 - val_gender_output_acc: 0.8807 - val_image_quality_output_acc: 0.5583 - val_age_output_acc: 0.3880 - val_weight_output_acc: 0.6458 - val_bag_output_acc: 0.5990 - val_footwear_output_acc: 0.6521 - val_pose_output_acc: 0.8401 - val_emotion_output_acc: 0.6880\n",
            "Epoch 14/100\n",
            "Learning rate:  0.0001\n",
            "180/180 [==============================] - 141s 783ms/step - loss: 5.9501 - gender_output_loss: 0.0749 - image_quality_output_loss: 0.9658 - age_output_loss: 1.2462 - weight_output_loss: 0.8654 - bag_output_loss: 0.7770 - footwear_output_loss: 0.5329 - pose_output_loss: 0.1304 - emotion_output_loss: 0.8276 - gender_output_acc: 0.9753 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4487 - weight_output_acc: 0.6612 - bag_output_acc: 0.6617 - footwear_output_acc: 0.7698 - pose_output_acc: 0.9640 - emotion_output_acc: 0.7161 - val_loss: 7.4006 - val_gender_output_loss: 0.3991 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.3928 - val_weight_output_loss: 0.9330 - val_bag_output_loss: 0.8622 - val_footwear_output_loss: 0.8656 - val_pose_output_loss: 0.5406 - val_emotion_output_loss: 0.9040 - val_gender_output_acc: 0.8807 - val_image_quality_output_acc: 0.5583 - val_age_output_acc: 0.3885 - val_weight_output_acc: 0.6458 - val_bag_output_acc: 0.5979 - val_footwear_output_acc: 0.6510 - val_pose_output_acc: 0.8401 - val_emotion_output_acc: 0.6880\n",
            "Epoch 15/100\n",
            "Learning rate:  0.0001\n",
            "180/180 [==============================] - 142s 787ms/step - loss: 5.9371 - gender_output_loss: 0.0706 - image_quality_output_loss: 0.9655 - age_output_loss: 1.2405 - weight_output_loss: 0.8632 - bag_output_loss: 0.7775 - footwear_output_loss: 0.5276 - pose_output_loss: 0.1332 - emotion_output_loss: 0.8293 - gender_output_acc: 0.9780 - image_quality_output_acc: 0.5567 - age_output_acc: 0.4532 - weight_output_acc: 0.6638 - bag_output_acc: 0.6588 - footwear_output_acc: 0.7752 - pose_output_acc: 0.9617 - emotion_output_acc: 0.7161 - val_loss: 7.4018 - val_gender_output_loss: 0.3990 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.3931 - val_weight_output_loss: 0.9331 - val_bag_output_loss: 0.8620 - val_footwear_output_loss: 0.8658 - val_pose_output_loss: 0.5417 - val_emotion_output_loss: 0.9040 - val_gender_output_acc: 0.8797 - val_image_quality_output_acc: 0.5589 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.6458 - val_bag_output_acc: 0.5990 - val_footwear_output_acc: 0.6505 - val_pose_output_acc: 0.8401 - val_emotion_output_acc: 0.6880\n",
            "Epoch 16/100\n",
            "Learning rate:  0.0001\n",
            "180/180 [==============================] - 143s 795ms/step - loss: 5.9491 - gender_output_loss: 0.0759 - image_quality_output_loss: 0.9672 - age_output_loss: 1.2447 - weight_output_loss: 0.8641 - bag_output_loss: 0.7788 - footwear_output_loss: 0.5285 - pose_output_loss: 0.1328 - emotion_output_loss: 0.8273 - gender_output_acc: 0.9763 - image_quality_output_acc: 0.5549 - age_output_acc: 0.4444 - weight_output_acc: 0.6639 - bag_output_acc: 0.6588 - footwear_output_acc: 0.7706 - pose_output_acc: 0.9616 - emotion_output_acc: 0.7161 - val_loss: 7.4011 - val_gender_output_loss: 0.3993 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.3932 - val_weight_output_loss: 0.9330 - val_bag_output_loss: 0.8620 - val_footwear_output_loss: 0.8659 - val_pose_output_loss: 0.5405 - val_emotion_output_loss: 0.9040 - val_gender_output_acc: 0.8802 - val_image_quality_output_acc: 0.5583 - val_age_output_acc: 0.3865 - val_weight_output_acc: 0.6458 - val_bag_output_acc: 0.5995 - val_footwear_output_acc: 0.6505 - val_pose_output_acc: 0.8411 - val_emotion_output_acc: 0.6880\n",
            "Epoch 17/100\n",
            "Learning rate:  5e-06\n",
            "180/180 [==============================] - 144s 797ms/step - loss: 5.9274 - gender_output_loss: 0.0651 - image_quality_output_loss: 0.9670 - age_output_loss: 1.2419 - weight_output_loss: 0.8601 - bag_output_loss: 0.7773 - footwear_output_loss: 0.5308 - pose_output_loss: 0.1263 - emotion_output_loss: 0.8291 - gender_output_acc: 0.9809 - image_quality_output_acc: 0.5543 - age_output_acc: 0.4503 - weight_output_acc: 0.6649 - bag_output_acc: 0.6611 - footwear_output_acc: 0.7713 - pose_output_acc: 0.9631 - emotion_output_acc: 0.7161 - val_loss: 7.4005 - val_gender_output_loss: 0.3988 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.3928 - val_weight_output_loss: 0.9329 - val_bag_output_loss: 0.8619 - val_footwear_output_loss: 0.8659 - val_pose_output_loss: 0.5409 - val_emotion_output_loss: 0.9040 - val_gender_output_acc: 0.8797 - val_image_quality_output_acc: 0.5583 - val_age_output_acc: 0.3870 - val_weight_output_acc: 0.6458 - val_bag_output_acc: 0.5984 - val_footwear_output_acc: 0.6510 - val_pose_output_acc: 0.8406 - val_emotion_output_acc: 0.6880\n",
            "Epoch 18/100\n",
            "Learning rate:  5e-06\n",
            "180/180 [==============================] - 142s 788ms/step - loss: 5.9332 - gender_output_loss: 0.0711 - image_quality_output_loss: 0.9663 - age_output_loss: 1.2391 - weight_output_loss: 0.8608 - bag_output_loss: 0.7768 - footwear_output_loss: 0.5319 - pose_output_loss: 0.1301 - emotion_output_loss: 0.8273 - gender_output_acc: 0.9786 - image_quality_output_acc: 0.5539 - age_output_acc: 0.4505 - weight_output_acc: 0.6634 - bag_output_acc: 0.6597 - footwear_output_acc: 0.7740 - pose_output_acc: 0.9633 - emotion_output_acc: 0.7162 - val_loss: 7.4013 - val_gender_output_loss: 0.3992 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.3929 - val_weight_output_loss: 0.9331 - val_bag_output_loss: 0.8621 - val_footwear_output_loss: 0.8661 - val_pose_output_loss: 0.5408 - val_emotion_output_loss: 0.9039 - val_gender_output_acc: 0.8812 - val_image_quality_output_acc: 0.5583 - val_age_output_acc: 0.3870 - val_weight_output_acc: 0.6458 - val_bag_output_acc: 0.5979 - val_footwear_output_acc: 0.6510 - val_pose_output_acc: 0.8417 - val_emotion_output_acc: 0.6880\n",
            "Epoch 19/100\n",
            "Learning rate:  5e-06\n",
            " 78/180 [============>.................] - ETA: 1:12 - loss: 5.9578 - gender_output_loss: 0.0681 - image_quality_output_loss: 0.9747 - age_output_loss: 1.2409 - weight_output_loss: 0.8623 - bag_output_loss: 0.7835 - footwear_output_loss: 0.5187 - pose_output_loss: 0.1425 - emotion_output_loss: 0.8373 - gender_output_acc: 0.9792 - image_quality_output_acc: 0.5449 - age_output_acc: 0.4411 - weight_output_acc: 0.6615 - bag_output_acc: 0.6486 - footwear_output_acc: 0.7881 - pose_output_acc: 0.9601 - emotion_output_acc: 0.7125"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-1165:\n",
            "Process ForkPoolWorker-1164:\n",
            "Process ForkPoolWorker-1168:\n",
            "Process ForkPoolWorker-1166:\n",
            "Process ForkPoolWorker-1167:\n",
            "Process ForkPoolWorker-1159:\n",
            "Process ForkPoolWorker-1162:\n",
            "Process ForkPoolWorker-1169:\n",
            "Process ForkPoolWorker-1163:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Process ForkPoolWorker-1158:\n",
            "Process ForkPoolWorker-1160:\n",
            "Process ForkPoolWorker-1161:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"<ipython-input-2-c7ea406dc13c>\", line 86, in __getitem__\n",
            "    image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"<ipython-input-2-c7ea406dc13c>\", line 86, in <listcomp>\n",
            "    image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-2-c7ea406dc13c>\", line 89, in __getitem__\n",
            "    image = self.augumentation.flow(image, shuffle=False, batch_size=batch).next()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-2-c7ea406dc13c>\", line 89, in __getitem__\n",
            "    image = self.augumentation.flow(image, shuffle=False, batch_size=batch).next()\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-2-c7ea406dc13c>\", line 89, in __getitem__\n",
            "    image = self.augumentation.flow(image, shuffle=False, batch_size=batch).next()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 448, in affine_transform\n",
            "    shape=output_shape)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/_ni_support.py\", line 75, in _get_output\n",
            "    output = numpy.zeros(shape, dtype=input.dtype.name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"<ipython-input-2-c7ea406dc13c>\", line 89, in __getitem__\n",
            "    image = self.augumentation.flow(image, shuffle=False, batch_size=batch).next()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_dtype.py\", line 319, in _name_get\n",
            "    def _name_get(dtype):\n",
            "  File \"<ipython-input-2-c7ea406dc13c>\", line 89, in __getitem__\n",
            "    image = self.augumentation.flow(image, shuffle=False, batch_size=batch).next()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f285a7e6167d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G80mLPjzKqZc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "8e8e1a69-ab52-482f-943e-caba34245fe5"
      },
      "source": [
        "Score = model.evaluate_generator(valid_gen)\n",
        "print([[a, b] for a,b in zip(df.columns,Score[9:])])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['gender', 0.8807291666666667], ['imagequality', 0.5583333333333333], ['age', 0.3875], ['weight', 0.6463541666666667], ['carryingbag', 0.5984375], ['footwear', 0.6510416666666666], ['emotion', 0.8411458333333334], ['bodypose', 0.6880208333333333]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}