{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Too_short Assgnment4_base.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maurya-anuj/Eip4/blob/master/Assignment4B/Assgnment4B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtlgM1BH5JmG",
        "colab_type": "code",
        "outputId": "43c1c0a1-c375-4bba-8b28-ef8c0c40d60a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64  # orig paper trained all networks with batch_size=128\n",
        "epochs = 50\n",
        "data_augmentation = True\n",
        "num_classes = 10\n",
        "\n",
        "# Subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "n = 3\n",
        "\n",
        "# Model version\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 1\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "\n",
        "# Model name, depth and version\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 45:\n",
        "        lr *= 0.5e-4\n",
        "    elif epoch > 25:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 15:\n",
        "        lr *= 0.5e-1\n",
        "    elif epoch > 5:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "########################################################\n",
        "########################################################\n",
        "pixel_level = False\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser\n",
        "\n",
        "########################################\n",
        "\n",
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=.05,\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               verbose=1,\n",
        "                               min_delta=0.01,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer]\n",
        "\n",
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model_info = model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # # set input mean to 0 over the dataset\n",
        "        # featurewise_center=False,\n",
        "        # # set each sample mean to 0\n",
        "        # samplewise_center=False,\n",
        "        # # divide inputs by std of dataset\n",
        "        # featurewise_std_normalization=False,\n",
        "        # # divide each input by its std\n",
        "        # samplewise_std_normalization=False,\n",
        "        # # apply ZCA whitening\n",
        "        # zca_whitening=False,\n",
        "        # # epsilon for ZCA whitening\n",
        "        # zca_epsilon=1e-06,\n",
        "        # # randomly rotate images in the range (deg 0 to 180)\n",
        "        # rotation_range=10,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # # set range for random shear\n",
        "        # shear_range=0.,\n",
        "        # # set range for random zoom\n",
        "        # zoom_range=0.1,\n",
        "        # # set range for random channel shifts\n",
        "        # channel_shift_range=0.,\n",
        "        # # set mode for filling points outside the input boundaries\n",
        "        # fill_mode='nearest',\n",
        "        # # value used for fill_mode = \"constant\"\n",
        "        # cval=0.,\n",
        "        # # randomly flip images\n",
        "        # horizontal_flip=True,\n",
        "        # # randomly flip images\n",
        "        # vertical_flip=False,\n",
        "        # # set rescaling factor (applied before any other transformation)\n",
        "        # rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=pixel_level),\n",
        "        # # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        # data_format=None,\n",
        "        # # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    model_info = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1,\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "# plot model history\n",
        "plot_model_history(model_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 16)   0           activation_1[0][0]               \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 16)   0           activation_3[0][0]               \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 32, 32, 16)   0           activation_5[0][0]               \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4640        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 32)   544         activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 32)   0           conv2d_10[0][0]                  \n",
            "                                                                 batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 32)   0           activation_9[0][0]               \n",
            "                                                                 batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 16, 16, 32)   0           activation_11[0][0]              \n",
            "                                                                 batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 32)   0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18496       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 64)     2112        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 64)     0           conv2d_17[0][0]                  \n",
            "                                                                 batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 8, 8, 64)     0           activation_15[0][0]              \n",
            "                                                                 batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 8, 8, 64)     0           activation_17[0][0]              \n",
            "                                                                 batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 64)     0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 64)     0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 64)           0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           650         flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 274,442\n",
            "Trainable params: 273,066\n",
            "Non-trainable params: 1,376\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v1\n",
            "Using real-time data augmentation.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/50\n",
            "782/782 [==============================] - 65s 83ms/step - loss: 1.6833 - acc: 0.4411 - val_loss: 1.9763 - val_acc: 0.4408\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.44080, saving model to /content/saved_models/cifar10_ResNet20v1_model.001.h5\n",
            "Epoch 2/50\n",
            "782/782 [==============================] - 57s 72ms/step - loss: 1.3231 - acc: 0.5793 - val_loss: 1.2544 - val_acc: 0.6041\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.44080 to 0.60410, saving model to /content/saved_models/cifar10_ResNet20v1_model.002.h5\n",
            "Epoch 3/50\n",
            "782/782 [==============================] - 57s 73ms/step - loss: 1.1619 - acc: 0.6407 - val_loss: 1.2074 - val_acc: 0.6416\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.60410 to 0.64160, saving model to /content/saved_models/cifar10_ResNet20v1_model.003.h5\n",
            "Epoch 4/50\n",
            "782/782 [==============================] - 56s 72ms/step - loss: 1.0543 - acc: 0.6813 - val_loss: 1.2771 - val_acc: 0.6418\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.64160 to 0.64180, saving model to /content/saved_models/cifar10_ResNet20v1_model.004.h5\n",
            "Epoch 5/50\n",
            "782/782 [==============================] - 56s 72ms/step - loss: 0.9811 - acc: 0.7087 - val_loss: 1.2304 - val_acc: 0.6605\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.64180 to 0.66050, saving model to /content/saved_models/cifar10_ResNet20v1_model.005.h5\n",
            "Epoch 6/50\n",
            "782/782 [==============================] - 58s 74ms/step - loss: 0.9296 - acc: 0.7275 - val_loss: 0.9506 - val_acc: 0.7238\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.66050 to 0.72380, saving model to /content/saved_models/cifar10_ResNet20v1_model.006.h5\n",
            "Epoch 7/50\n",
            "782/782 [==============================] - 58s 74ms/step - loss: 0.8835 - acc: 0.7454 - val_loss: 0.9764 - val_acc: 0.7258\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.72380 to 0.72580, saving model to /content/saved_models/cifar10_ResNet20v1_model.007.h5\n",
            "Epoch 8/50\n",
            "782/782 [==============================] - 58s 74ms/step - loss: 0.8489 - acc: 0.7580 - val_loss: 1.2180 - val_acc: 0.6650\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.72580\n",
            "Epoch 9/50\n",
            "782/782 [==============================] - 58s 74ms/step - loss: 0.8215 - acc: 0.7699 - val_loss: 0.8847 - val_acc: 0.7474\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.72580 to 0.74740, saving model to /content/saved_models/cifar10_ResNet20v1_model.009.h5\n",
            "Epoch 10/50\n",
            "782/782 [==============================] - 58s 74ms/step - loss: 0.7937 - acc: 0.7782 - val_loss: 0.9399 - val_acc: 0.7400\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.74740\n",
            "Epoch 11/50\n",
            "782/782 [==============================] - 58s 74ms/step - loss: 0.7690 - acc: 0.7873 - val_loss: 0.9584 - val_acc: 0.7340\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.74740\n",
            "Epoch 12/50\n",
            "782/782 [==============================] - 58s 74ms/step - loss: 0.7499 - acc: 0.7937 - val_loss: 0.7952 - val_acc: 0.7786\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.74740 to 0.77860, saving model to /content/saved_models/cifar10_ResNet20v1_model.012.h5\n",
            "Epoch 13/50\n",
            "782/782 [==============================] - 57s 73ms/step - loss: 0.7351 - acc: 0.8023 - val_loss: 0.8058 - val_acc: 0.7905\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.77860 to 0.79050, saving model to /content/saved_models/cifar10_ResNet20v1_model.013.h5\n",
            "Epoch 14/50\n",
            "782/782 [==============================] - 58s 74ms/step - loss: 0.7200 - acc: 0.8061 - val_loss: 0.7774 - val_acc: 0.7988\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.79050 to 0.79880, saving model to /content/saved_models/cifar10_ResNet20v1_model.014.h5\n",
            "Epoch 15/50\n",
            "782/782 [==============================] - 57s 73ms/step - loss: 0.7072 - acc: 0.8109 - val_loss: 0.7911 - val_acc: 0.7930\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.79880\n",
            "Epoch 16/50\n",
            "782/782 [==============================] - 57s 73ms/step - loss: 0.6893 - acc: 0.8196 - val_loss: 1.0054 - val_acc: 0.7472\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.79880\n",
            "Epoch 17/50\n",
            "782/782 [==============================] - 57s 73ms/step - loss: 0.6809 - acc: 0.8230 - val_loss: 0.9515 - val_acc: 0.7586\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.79880\n",
            "Epoch 18/50\n",
            "782/782 [==============================] - 58s 74ms/step - loss: 0.6685 - acc: 0.8268 - val_loss: 0.6948 - val_acc: 0.8239\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.79880 to 0.82390, saving model to /content/saved_models/cifar10_ResNet20v1_model.018.h5\n",
            "Epoch 19/50\n",
            "782/782 [==============================] - 59s 75ms/step - loss: 0.6572 - acc: 0.8310 - val_loss: 0.7791 - val_acc: 0.8052\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.82390\n",
            "Epoch 20/50\n",
            "782/782 [==============================] - 59s 75ms/step - loss: 0.6451 - acc: 0.8344 - val_loss: 0.7687 - val_acc: 0.8076\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.82390\n",
            "Epoch 21/50\n",
            "782/782 [==============================] - 59s 75ms/step - loss: 0.6435 - acc: 0.8373 - val_loss: 0.8633 - val_acc: 0.7779\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.82390\n",
            "Epoch 22/50\n",
            "782/782 [==============================] - 59s 75ms/step - loss: 0.6342 - acc: 0.8402 - val_loss: 0.9429 - val_acc: 0.7664\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.82390\n",
            "Epoch 23/50\n",
            "782/782 [==============================] - 59s 75ms/step - loss: 0.6311 - acc: 0.8424 - val_loss: 0.7602 - val_acc: 0.8101\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.82390\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
            "Epoch 24/50\n",
            "782/782 [==============================] - 59s 75ms/step - loss: 0.5367 - acc: 0.8774 - val_loss: 0.6022 - val_acc: 0.8595\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.82390 to 0.85950, saving model to /content/saved_models/cifar10_ResNet20v1_model.024.h5\n",
            "Epoch 25/50\n",
            "782/782 [==============================] - 58s 74ms/step - loss: 0.5067 - acc: 0.8872 - val_loss: 0.5822 - val_acc: 0.8673\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.85950 to 0.86730, saving model to /content/saved_models/cifar10_ResNet20v1_model.025.h5\n",
            "Epoch 26/50\n",
            "782/782 [==============================] - 59s 75ms/step - loss: 0.4958 - acc: 0.8905 - val_loss: 0.5767 - val_acc: 0.8688\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.86730 to 0.86880, saving model to /content/saved_models/cifar10_ResNet20v1_model.026.h5\n",
            "Epoch 27/50\n",
            "782/782 [==============================] - 59s 75ms/step - loss: 0.4830 - acc: 0.8958 - val_loss: 0.5828 - val_acc: 0.8677\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.86880\n",
            "Epoch 28/50\n",
            "782/782 [==============================] - 59s 75ms/step - loss: 0.4772 - acc: 0.8959 - val_loss: 0.5633 - val_acc: 0.8740\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.86880 to 0.87400, saving model to /content/saved_models/cifar10_ResNet20v1_model.028.h5\n",
            "Epoch 29/50\n",
            "782/782 [==============================] - 58s 75ms/step - loss: 0.4710 - acc: 0.8976 - val_loss: 0.5770 - val_acc: 0.8691\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.87400\n",
            "Epoch 30/50\n",
            "782/782 [==============================] - 58s 74ms/step - loss: 0.4680 - acc: 0.8965 - val_loss: 0.5659 - val_acc: 0.8693\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.87400\n",
            "Epoch 31/50\n",
            "782/782 [==============================] - 58s 75ms/step - loss: 0.4585 - acc: 0.9008 - val_loss: 0.5697 - val_acc: 0.8732\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.87400\n",
            "Epoch 32/50\n",
            "782/782 [==============================] - 59s 75ms/step - loss: 0.4529 - acc: 0.9035 - val_loss: 0.5564 - val_acc: 0.8757\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.87400 to 0.87570, saving model to /content/saved_models/cifar10_ResNet20v1_model.032.h5\n",
            "Epoch 33/50\n",
            "782/782 [==============================] - 59s 75ms/step - loss: 0.4481 - acc: 0.9030 - val_loss: 0.5726 - val_acc: 0.8702\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.87570\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 2.5000001187436284e-06.\n",
            "Epoch 34/50\n",
            "782/782 [==============================] - 59s 75ms/step - loss: 0.4410 - acc: 0.9072 - val_loss: 0.5604 - val_acc: 0.8737\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.87570\n",
            "Epoch 35/50\n",
            "782/782 [==============================] - 59s 75ms/step - loss: 0.4401 - acc: 0.9061 - val_loss: 0.5597 - val_acc: 0.8746\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.87570\n",
            "Epoch 36/50\n",
            "782/782 [==============================] - 59s 75ms/step - loss: 0.4366 - acc: 0.9076 - val_loss: 0.5596 - val_acc: 0.8741\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.87570\n",
            "Epoch 37/50\n",
            "782/782 [==============================] - 59s 75ms/step - loss: 0.4441 - acc: 0.9047 - val_loss: 0.5607 - val_acc: 0.8749\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.87570\n",
            "Epoch 38/50\n",
            "782/782 [==============================] - 58s 75ms/step - loss: 0.4391 - acc: 0.9061 - val_loss: 0.5589 - val_acc: 0.8754\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.87570\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 5e-07.\n",
            "Epoch 39/50\n",
            "782/782 [==============================] - 58s 75ms/step - loss: 0.4396 - acc: 0.9070 - val_loss: 0.5571 - val_acc: 0.8757\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.87570\n",
            "Epoch 40/50\n",
            "782/782 [==============================] - 58s 75ms/step - loss: 0.4424 - acc: 0.9051 - val_loss: 0.5577 - val_acc: 0.8747\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.87570\n",
            "Epoch 41/50\n",
            "782/782 [==============================] - 59s 75ms/step - loss: 0.4371 - acc: 0.9076 - val_loss: 0.5581 - val_acc: 0.8753\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.87570\n",
            "Epoch 42/50\n",
            "782/782 [==============================] - 59s 76ms/step - loss: 0.4361 - acc: 0.9080 - val_loss: 0.5582 - val_acc: 0.8752\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.87570\n",
            "Epoch 43/50\n",
            "782/782 [==============================] - 59s 76ms/step - loss: 0.4405 - acc: 0.9063 - val_loss: 0.5599 - val_acc: 0.8753\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.87570\n",
            "Epoch 44/50\n",
            "782/782 [==============================] - 59s 76ms/step - loss: 0.4400 - acc: 0.9057 - val_loss: 0.5580 - val_acc: 0.8755\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.87570\n",
            "Epoch 45/50\n",
            "782/782 [==============================] - 59s 76ms/step - loss: 0.4400 - acc: 0.9071 - val_loss: 0.5607 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.87570\n",
            "Epoch 46/50\n",
            "782/782 [==============================] - 59s 76ms/step - loss: 0.4398 - acc: 0.9067 - val_loss: 0.5584 - val_acc: 0.8746\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.87570\n",
            "Epoch 47/50\n",
            "782/782 [==============================] - 58s 75ms/step - loss: 0.4398 - acc: 0.9065 - val_loss: 0.5619 - val_acc: 0.8739\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.87570\n",
            "Epoch 48/50\n",
            "782/782 [==============================] - 58s 74ms/step - loss: 0.4347 - acc: 0.9088 - val_loss: 0.5588 - val_acc: 0.8753\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.87570\n",
            "Epoch 49/50\n",
            "782/782 [==============================] - 58s 74ms/step - loss: 0.4412 - acc: 0.9061 - val_loss: 0.5568 - val_acc: 0.8749\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.87570\n",
            "Epoch 50/50\n",
            "782/782 [==============================] - 58s 74ms/step - loss: 0.4406 - acc: 0.9073 - val_loss: 0.5587 - val_acc: 0.8752\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.87570\n",
            "10000/10000 [==============================] - 3s 322us/step\n",
            "Test loss: 0.5587193824768066\n",
            "Test accuracy: 0.8752\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFNCAYAAABSRs15AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRERERoj/DydM4zwXfF5gAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}